{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Block Model experiments\n",
    "As we have seen in previous experiments, Tangles do not outperform a simple SOE-kMeans baseline, when applied to euclidean data. This can be expected, as kMeans has a euclidean bias, which could help recover the original data. To see if tangles perform better on non-euclidean data, we will use a stochastic block model (SMB).\n",
    "\n",
    "## Stochastic Block Model\n",
    "An SMB is a graph model. For the simple models we use here, we have that each SMB consists of $k$ clusters with $n$ points each. There exists an edge between two points $v, w$ with probability $p$ if the points are in the same cluster, and an edge with probability $q$ if they are in different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from data_generation import generate_smb_data\n",
    "from estimators import OrdinalTangles\n",
    "from questionnaire import Questionnaire\n",
    "from baselines import soe_knn_baseline\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import networkx as nx\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(120384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are providing a function to easily evaluate tangles performance on SMB. We \n",
    "build an SMB, generate triplets from it via shortest path, run the baseline as well as tangles on it, and give back the results averaged over a number of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smb_performance(p, q, n=20, k=5, n_runs=10):\n",
    "    \"\"\"\n",
    "    Tests the performance of tangles on an SMB. \n",
    "    Returns the score of the the tangles algorithm and of the baseline (SOE-kMeans) \n",
    "    average over n_runs.\n",
    "    \"\"\"\n",
    "    tangles_scores = []\n",
    "    baseline_scores = []\n",
    "    for _ in range(n_runs):\n",
    "        graph, ys = generate_smb_data(n=n, k=k, p=p, q=q)\n",
    "        questionnaire = Questionnaire.from_graph(graph, density=0.1)\n",
    "\n",
    "        triplets = questionnaire.values\n",
    "        triplets_cblearn = questionnaire.to_bool_array()\n",
    "\n",
    "        tangles = OrdinalTangles(8, verbose=1)\n",
    "        tangles_labels = tangles.fit_predict(triplets)\n",
    "        tangles_score = normalized_mutual_info_score(ys, tangles_labels)\n",
    "\n",
    "        #\n",
    "        soe_knn = soe_knn_baseline(2, 5)\n",
    "        baseline_labels = soe_knn.fit_predict(*triplets_cblearn)\n",
    "        baseline_score = normalized_mutual_info_score(ys, baseline_labels)\n",
    "\n",
    "        baseline_scores.append(baseline_score)\n",
    "        tangles_scores.append(tangles_score)\n",
    "\n",
    "    return np.mean(tangles_scores), np.mean(baseline_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smoke test to see if everything runs and has expected results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangles_nmi, baseline_nmi = smb_performance(0.9, 0.1)\n",
    "print(f\"Tangles: {tangles_nmi}\\nBaseline: {baseline_nmi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we build a meshgrid over different parameter values and plot it in a heat map. This is similar to what [Solveig did in her SMB experiments](https://arxiv.org/abs/2006.14444) (but there she used [Kernighan-Lin](https://en.wikipedia.org/wiki/Kernighanâ€“Lin_algorithm) to find cuts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meshgrid\n",
    "p, q = np.meshgrid(np.arange(0.1, 1.0, 0.1), np.concatenate(\n",
    "    ([0.01], np.arange(0.05, 0.5, 0.05))))\n",
    "\n",
    "tangles_nmi_grid = np.zeros_like(p)\n",
    "baseline_nmi_grid = np.zeros_like(q)\n",
    "for i in range(p.shape[0]):\n",
    "    for j in range(p.shape[1]):\n",
    "        nmi_score, baseline_score = smb_performance(p[i, j], q[i, j])\n",
    "        tangles_nmi_grid[i, j] = nmi_score\n",
    "        baseline_nmi_grid[i, j] = baseline_score\n",
    "\n",
    "tangles_nmi_df = pd.DataFrame(\n",
    "    {'p': p.ravel(), 'q': q.ravel(), 'nmi': tangles_nmi_grid.ravel()})\n",
    "baseline_nmi_df = pd.DataFrame(\n",
    "    {'p': p.ravel(), 'q': q.ravel(), 'nmi': baseline_nmi_grid.ravel()})\n",
    "\n",
    "tangles_chart = alt.Chart(tangles_nmi_df).mark_rect().encode(\n",
    "    x=alt.X('p:O', axis=alt.Axis(title='p', format=\".2\")),\n",
    "    y=alt.Y('q:O', axis=alt.Axis(title='q', format=\".2\"),\n",
    "            sort=alt.EncodingSortField('q', order='descending'),\n",
    "            ),\n",
    "    color='nmi:Q'\n",
    ").properties(title=\"Tangles\").interactive()\n",
    "\n",
    "baseline_chart = alt.Chart(baseline_nmi_df).mark_rect().encode(\n",
    "    x=alt.X('p:O', axis=alt.Axis(title='p', format=\".2\")),\n",
    "    y=alt.Y('q:O', axis=alt.Axis(title='q', format=\".2\"),\n",
    "            sort=alt.EncodingSortField('q', order='descending'),\n",
    "            ),\n",
    "    color='nmi:Q'\n",
    ").properties(title=\"Baseline\").interactive()\n",
    "\n",
    "(baseline_chart | tangles_chart).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we observe are a bit weird, we would expect Tangles to outperform the baseline. This seems to require further inquiries. We suspect that shortest path length might not be the best candidate, as out-of-cluster connections are quite prevalent, and due to there being just a lot more out-of-cluster nodes than in-cluster nodes, the possibility that a direct connection to an out-of-cluster node is existing is very high.\n",
    "\n",
    "We use a simple MC-estimate to determine inner-path expected length, outer-path expected length and based on that, number of nodes that have an outer-cluster-node as nearest neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inner distance: 1.25\n",
      "Average outer distance: 1.7154166666666668\n",
      "Average percentage of closest neighbour being outer cluster: 0.95\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "k = 5\n",
    "graph, ys = generate_smb_data(n=n, k=k, p=0.7, q=0.2)\n",
    "paths = nx.floyd_warshall_numpy(graph)\n",
    "\n",
    "inner_dists_list = []\n",
    "outer_dists_list = []\n",
    "num_wrong_neighbour_list = []\n",
    "\n",
    "# average path length\n",
    "for i in range(k):\n",
    "    block_lower, block_upper = (i * n, (i + 1) * n)\n",
    "    current_block_mask = np.ones_like(paths) == 0\n",
    "    current_block_mask[block_lower:block_upper, block_lower:block_upper] = True\n",
    "    inner_dists = paths[current_block_mask]\n",
    "    outer_dists = paths[~current_block_mask]\n",
    "\n",
    "    inner_dists_list.append(inner_dists.mean())\n",
    "    outer_dists_list.append(outer_dists.mean())\n",
    "\n",
    "    current_points = np.copy(paths[block_lower:block_upper, :])\n",
    "    current_points[current_points == 0] = np.inf\n",
    "    nearest_neighbours = np.argmin(current_points, axis=1)\n",
    "    num_wrong_neighbour_list.append(np.sum(np.logical_or(nearest_neighbours <=\n",
    "                                                         block_lower, nearest_neighbours >= block_upper))/n)\n",
    "\n",
    "print(f\"Average inner distance: {np.mean(inner_dists_list)}\")\n",
    "print(f\"Average outer distance: {np.mean(outer_dists_list)}\")\n",
    "print(\n",
    "    f\"Average percentage of closest neighbour being outer cluster: {np.mean(num_wrong_neighbour_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the average inner vs outer distance is different, but not hugely. More concerning however, is the number of nodes with wrong cluster. We cannot expect to achieve any reasonable clustering with that (and it's suspicious that SOE-kMeans performs that well still, this might be due to induced bias, since we tell the algorithm the number of clusters).\n",
    "\n",
    "We also find another peculiarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(num_wrong_neighbour_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all but the first cluster have their closest neighbour in another cluster. This is an artifact of the argmax. When multiple values in an array are the same, argmax always picks the first occurence. Additionally, for all clusters besides the first, there is a very high probability, that there is already a direct neighbour in the first cluster, for example with $q = 0.2$ and $n = 20$:\n",
    "\n",
    "$$1 - (1-q)^n = 1 - 0.8^{20} = 0.0115$$ \n",
    "\n",
    "This causes argmax to almost always pick a random node from the first cluster."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3df0bc9139dfbbc2bb43367c6e0b1c6fcd610288dc61cb8c73318fe0a42d151"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tangles-thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
