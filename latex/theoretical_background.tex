\chapter{Theoretical Background}\label{theory}
Here we write about the theoretical background.

\section{Tangles}\label{theory:tangles}
% What are Tangles? How are they used? What advantages do they
% prove?
% TODO: How to cite correctly here?
Tangles have been a tool in mathematical graph theory, introduced originally by \cite{robertsonGraphMinorsObstructions1991} 
with a diverse range of application %TODO: Citations from Solveig. \\

In recent times, through the work of \cite{klepperClusteringTanglesAlgorithmic2021}, they have been successfully applied
to solve problems of clustering. The mentioned work delivers an algorithmic framework and theoretical guarantees for basic problem settings.
Additionally, it delivers simplified notations, adapted to the domain of computer science. When talking about Tangles,
we will exclusively use the definitions introduced there, not those that might be common in mathematics. \\

In this section, we will now deliver a very brief recap of the basic notions, theory and applications of Tangles in a clustering context.
For more in-depth explanations of the algorithms and exact procedures, refer to \cite{klepperClusteringTanglesAlgorithmic2021}.
\subsection{What is a Tangle?}
The central object in Tangles Theory is a \textbf{bipartition} (which we also refer to as a cut). 
A bipartition is simply a way of dividing a set of elements $V =  \{ v_1, v_2, \ldots \}$ into two distinct subsets $A, B \subset V$, such that
$A \cap B = \O$ and $A \cup B = V$. We can also write a bipartition as $P = \{A, \overline{A}\}$, with $A \subset V$ and $\overline{A}$ being the
complement of $A$ with respect to $V$. \\

For such a bipartition to be useful in clustering, we expect it to hold some degree of information about the cluster 
structure of our data. This means that a good bipartition should not separate groups of data that are tightly coupled.
If we imagine a graph data structure, a good bipartition $P = \{A, \overline{A}\}$ might be a separation of the set of nodes $V$ such that there 
are only a few edges between $A$ and $\overline{A}$. How useful a cut might be for our clustering will be quantified through a \textbf{cost function} 
$c: \mathcal{P}(V) \to \R$, with $\mathcal{P}(V)$ denoting the power set of V. One is free to choose this cost function and it might be dependent on the problem at hand. 
%TODO: Figure of bipartiion
%TODO: insert cost function used?
\\
Assume that for a set of elements $V$ that we are equipped with a set of bipartitions $\mathcal{B} = \{\{A_1, \overline{A_1}\}, \ldots, \{A_n, \overline{A_n}\} \} $ on $V$.
Coupled with the cost function, this set of bipartitions should tell us a lot about the cluster structure of the data:
we know for all bipartitions, how much they do or don't separate dense regions in $V$. The task of the Tangles framework is to aggregate
the information present in the bipartitions and bring it into a useable form. For this, we process $\mathcal{B}$ to a set of so-called \textbf{Tangles}, which
correspond to specific ways of orienting the cuts in a consistent way such that they point to cohesive structures in the data. 
Orienting here means that we pick one specific side of a bipartition. An \textbf{Orientation} of $\mathcal{B}$ is then a set $O = \{o_1, o_2, \ldots o_n\}$, where $o_i$ 
corresponds to either the partition $A_i$ (oriented \textit{left}) or $\overline{A_i}$ (oriented \textit{right}). A consistent orientation (which we also call a Tangle) is an orientation for which:

\begin{align}
    \forall A,B,C \in O: \left| A \cap B \cap B \right| \ge a
.\end{align}
for some fixed parameter $a \in \N$, which we refer to as \textbf{agreement} parameter. This point is also where we need the cost function: Without it, a lot
of reasonably sized sets of bipartitions wouldn't allow for any tangles, as there are simply too many of them to consistently orient. Imagine if our set of bipartitions would 
contain a few random bipartitions: on average, each of these cuts our set of points in half, so we can at most consistently orient on the order of $O(\log(n))$ many of them.
Using the cost function, we can simply restrict our tangles to a set of low-cost (and thus very insightful) cuts $P_{\psi}$, using a threshhold $\psi \in \R$ such that
$P_{\psi} = \{ P c(P) \le \psi \}$. A tangle on $P_{\psi}$ is then said to have order $\psi$.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tangles-example.pdf}
    \caption{An example of how a simple tangle might look like, if we assume a reasonably sized agreement (say $a = 3$). The red lines represent simple cuts, which divide the sets of points into a bipartition.
    A tangle on this set of bipartitions might orient all bipartitions to the left side (indicated by the arrow), so that they point to the dense structure there. Another 
possible tangle might orient all cuts to the left. Notice that a tangle on this set of bipartitions can only either point all bipartitions to the left or to the right, else
the consistency criterion is violated. This might already give a good intuition on why tangles are able to find dense structures in data.}
    \label{fig:tangles-example}
\end{figure}
\FloatBarrier

\subsection{Processing Tangles to a clustering}
As we have seen in \autoref{fig:tangles-example}, a tangle might correspond directly to a cluster. But, a given set of bipartitions usually allows for a wide 
variety of possible tangles, some of them pointing to different or overlapping clusters. We now have to process this set of tangles into a useable clustering.
This step is a bit involved and we aim to only give a rough, intuitional overview here. \\

Given a set of bipartitions $\mathcal{B} = \{b_1, b_2, \ldots b_n\} $, we first want to determine for all possible orders $\psi$ the sets of all tangles of order $\psi$ on $\mathcal{B}$ 
according to a given cost function $c$. Inuitively, the order of the set of tangles determines how coarse the clustering is they define:
If we only use bipartitions with a low cost (so in the case of small $\psi$), then the bipartitions cut only very loosely connected structures. 
If the cost is higher, the bipartitions are allowed to cut through more dense regions. This directly induces a sort of hierarchy, where we go from coarse cluster
structures to fine ones with increasing $\psi$. \\
To better handle this procedure computationally, we build a tree structure on the set of tangles, called the \textbf{Tangle Search Tree}. In the tangles search tree, 
one node represents a possible tangle. Every level of the tree contains all possible tangles of a certain threshhold $\psi_i$ which directly corresponds to the cost of bipartition $b_i$. 
The exact makeup of the tangle is determined by walking the path from the root to the node, and adding bipartition $b_i$ to the tangle in a left-oriented way, if it is a left child and
in a right-oriented way if it is a right child. An exemplary tangle search tree is illustrated in \autoref{fig:tangles-tree-example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tangles-tree-example.png}
    \caption{An example of a possible tangles search tree for a set of bipartitions $\mathcal{B} = \{\{A_1, \overline{A_1}\}, \{A_2, \overline{A_2}\}, \{A_3, \overline{A_3}\} \}$. 
        Each level corresponds to the tangles of the order given by the bipartition $P_i$ that is indicated to the left of it.
        Let us take a look at the sole node in level 3. If we would want to find out, which orientations the corresponding tangle $T$ consists of, we just walk from
        the root to it and add the bipartitions in the direction indicated by the tree. We end up with $T = \{\overline{A_1}, \overline{A_2}, A_3\}$. 
        Figure taken with permission from \cite{klepperClusteringTanglesAlgorithmic2021}.}
    \label{fig:tangles-tree-example}
\end{figure}

We can now obtain a soft, hierarchical clustering from this tangle search tree. For this, the interesting nodes are those
where the tree splits up (as this represents a new clustering) and the leaves (which correspond to the clusters). 
For each of the \textit{splitting nodes}, we determine the set of \textit{characterizing cuts}.
A cut belongs to this set, if it is both oriented the same way inside the subtrees, and if it is oriented in a different way between the two subtrees.
To illustrate this, we take a look at the exemplary tree in \autoref{fig:tangles-tree-example}. Here, for the root node, $P_1$ is a characterizing cut (pretty trivially),
while $P_2$ is not: below the node $A_1$, the bipartition is both oriented to the left and to to the right, violating the requirement that the cuts are always oriented
the same way inside the subtrees. 

The characterizing cuts express some kind of agreement on how to align the bipartitions in the subtrees of the node, which we can leverage 
for a soft clustering. If we now want to determine, with what probability a point $a$ belongs to a cluster $C$ represented by a leaf in the tree, we simply walk down 
the tree from the root, and count at every splitting node how many characterizing cuts contain the point $a$, and how many don't. Normalized by the total amount of
characterizing cuts, we can interpret this as a probability to walk down either the left or the right path. To get a total probability that $a$ belongs to $C$, 
we simply multiply the probabilities that we obtain on the path to $C$ at every splitting node. The corresponding hard clustering is obtained by assigning each
node to the cluster it belongs to with the highest probability.
    
% Given a set of bipartitions $\mathcal{B}$, we first sort the bipartitions according to their costs given by the cost function $c$. 
% We now iterate over the set of bipartitions, while building up a binary tree in a level-wise fashion. For every bipartition $P_i$, 
% we add it to each node in level $i-1$, if it is consistent with the tangle inducded by the given node. 

\section{Ordinal Constraints}
A dataset of ordinal constraints consists of object for which we don't know their exact attributes or features, but only how they relate to other objects
in the form of comparisons such as \textit{item $i$ is closer to item $j$ than to item $k$}. Formally, we assume that $i, j, k$ are from 
a set $D$ where we can define a dissimilarity function $d: D \times D \to \R$. Note that $d$ which can be a metric on $D$, but does not have to be.
We can then express our above constraint as $d(i, j) < d(i, k)$. \\

Such data often appears when humans are asked to judge objects, as they are often naturally perform better on comparing objects
than on accurately placing them on an abstract scale \cite{stewartAbsoluteIdentificationRelative2005}. Applications are for example estimation of perceptual scales in psychophysics 
\cite{haghiriEstimationPerceptualScales2020} or crowd-sourcing clustering algorithms \cite{ukkonenCrowdsourcedCorrelationClustering2017}. 
We want to keep our focus here mainly the realm of psychophysics.


\subsection{Formats of Ordinal Constraints}
%TODO: Explain d as metric
Ordinal data usually takes two possible forms. 
The more general form of an ordinal data constraint is the quadruplet form, for example used in \cite{ghoshdastidarFoundationsComparisonBasedHierarchical2019}. 
Here, we have constraints such as $\text{d}(a,b) < \text{d}(c,d)$. A more commonly encountered form is the triplet form, which is simply a quadruplet where $a = c$, such as
$\text{d}(a,b) < \text{d}(a,d)$, used for example in \cite{vankadaraInsightsOrdinalEmbedding2021,haghiriComparisonBasedFrameworkPsychophysics2019}. We refer to datasets
consisting of triplet comparisons as \textit{triplet data}.
To obtain such triplet data, one might present to human participants three images $a, b, d$, with image $a$ as anchor point and ask them, which of the images $b,d$ are closer to $a$. \\

The way that participants are questioned is often varied on the context of the experiment (and might also influence their answers). An
example is presenting the participant with three images, and asking \textit{which is the most central image?}, or \textit{which is the odd one out?},
but the results are then always transformed back to triplet format for further prcoessing. For example, if $a$ is the \textit{odd-one-out} of the three elements $a,b,c$, 
then we know that $d(b,c) < d(b,a)$ and $d(c, b) < d(c,a)$. 

\subsection{Ordinal embeddings}
One of the most central problems consists of finding a so called \textit{ordinal embedding} of the data. If we have a set of triplet comparisons $T = \{t_1, t_2, \ldots t_n\}$, 
of the form $t_i = \left( a,b,c \right)$, encoding that $d(a,b) < d(a,c)$, 
we want to find a set of points $y_1, y_2, \ldots y_n \in \R^{m}$, such that they uphold the most of the original triplet constraints in $\R^{m}$ with the euclidean distance
as metric. More formally, we want to minimize \cite{vankadaraInsightsOrdinalEmbedding2021}:
\[
    \min_{y_1, \ldots y_n \in \R^{m}} \sum_{t=\left( i,j,k \right)  \in T} \mathbbm{1}_{ \|y_i - y_j\| < \|y_i - y_k\| }
.\] 
This problem is difficult to optimize and thus various algorithms have been propose that solve a relaxed or modified version of this objective function. 
These include Soft Ordinal Embedding (SOE) \cite{teradaLocalOrdinalEmbedding2014}, Generalized Non-Metric Multidimensional Scaling (GNDMS) \cite{agarwalGeneralizedNonmetricMultidimensional2007}, 
Crowd Kernel Learning (CKL) \cite{tamuzAdaptivelyLearningCrowd2011}, Fast Ordinal Triplets Embedding (FORTE) \cite{jainFiniteSamplePrediction2016},
T-Stochatic Triplet Embedding (T-STE) \cite{laurensvandermaatenStochasticTripletEmbedding2012} and various others. \\

It has been proven that if the original points come from the space $\R^{m}$, one can recover the points (up to scaling and orthogonal transformations) with $O(mn\log(n)$ triplet comparisons
\cite{jainFiniteSamplePrediction2016}. On this basis, an ordinal embedding can be used together with more classical machine learning algorithms, such as support vector machines or k-Means,
for other machine learning tasks. This approach has for example been used for classification \cite{tamuzAdaptivelyLearningCrowd2011, kleindessnerLensDepthFunction2017} and for clustering \cite{kleindessnerLensDepthFunction2017} on ordinal data. \\

%TODO This might better be something like "methods"?
\section{Applying Tangles to Triplet Data}
As we have made out in \autoref{theory:tangles}, the tangles algorithm operates on bipartitions of data that contain some information about the cluster structure.
If we have obtained a set of triplet data, we are now faced with the task of processing this data to appropriate bipartitions. In this work, we have developed two
methods for this which we call \textit{landmark cuts} and \textit{majority cuts}. We will elaborate on the methods and intuitions in the following sections.

\subsection{Landmark cuts}\label{theory:landmark_cuts}
In recent years, there have been algorithms that hope to speed up ordinal embedding by focussing on so-called \textit{landmarks}\cite{ghoshLandmarkOrdinalEmbedding2019, andertonScalingOrdinalEmbedding2019}, which are objects in the dataset that for which we know all (or all relevant) triplet comparisons.
The definitions of what constitutes landmarks varies a bit, but we will use one that is used in \cite{haghiriComparisonBasedFrameworkPsychophysics2019}.
Assume we have a set of objects $D$, as well as a set of $T$ triplet constraints of the form $(a,b,c)$, indicating that $d(a,b) < d(a,c)$. In a landmark setting, we have a set 
of $m$ objects $L \subset D$, for which $\forall l_1, l_2 \in L \forall x \in D: (x, l_1, l_2) \in T \vee (x, l_2, l_1) \in T$. This admits to a very natural notion of bipartitions: 
for each combination of landmarks $l_1, l_2$, we can make a bipartition $P = \{A, \overline{A}\}$ by assigning all points closer to $l_1$ to $A$ and all 
those closer to $l_2$ to $\overline{A}$. Thus we can define $A_{12} = \{ x \in D \mid \left( x, l_1, l_2 \right) \in T \}$, denoting as $P_L = \{A_ij  \mid i, j \in 1\ldots m\}, i < j\}$ 
all such bipartitions on $L$. These bipartitions are then called \textit{landmark cuts}.
Later in the simulations, as tangles is not reliant on having all triplet constraints for all landmark objects, we will simply sample a subset $P'_{L} \subset P_L$ of bipartitions,
which corresponds to repeatedly picking some objects $a, b$ and sampling all triplet comparisons to all other objects $x \in D$. \\

These landmark cuts intuitively capture a cluster structure: the more close $x$ is to $l_1$ according to $d$, the more likely that $A_{12}$ will contain $x$. In the end, $A_{12}$ will
consist of the points that are in some sense close to $l_1$. In the euclidean space, this notion is very easily captured: A landmark cut between $l_1, l_2$ is
simply a linear cut between the two points, as illustrated in \autoref{theory:landmark_cuts}.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/landmark_cut.png}
        \caption{Example of a simple landmark cut, with the two triangles being the landmarks $l_1, l_2$, where all red items (left of the line) 
        would be assigned to $A$, all blue items to $\overline{A}$ (or equivalently the other way around).}
        \label{fig:landmark_cut}
    \end{figure}

The landmark approach is a quite unusual way of sampling triplet questions.
In most triplet experiments, the triplets to sample are chosen randomly \cite{kleindessnerLensDepthFunction2017, haghiriEstimationPerceptualScales2020} 
or according to some metric (f.e. maximizing some measure of gained information) \cite{roadsEnrichingImageNetHuman2021}. 
There was no experimental dataset available which is sampled according to a landmark approach, where the objects exhibit a cluster structure, thus we rely on simulations
for testing our methods.

\subsection{Majority cuts}
As explained in \autoref{theory:landmark_cuts}, sampling triplet data in a landmark-fashion is not very widely used in current practice. Due to this, we aimed
to also develop a more general approach to processing tangles to cuts that can be applied to any set of triplet comparisons $T$ regardless of the sampling method.
For this, we again use the intuition that triplets tell us something about the closeness of data point, and thus about the cluster structure. Assume again
that we have a set of objects $D$ and a set of triplet comparisons $T$. We fix two points $a,b \in D$. Assume that $a,b$ are close and we and sample a random point $x$,
then, with high probability, it will be that $d(a,b) < d(a,x)$. The reverse holds, if $a,b$ are far away. Now we can take this the other way around: 
if we observe a triplet $(a,b,c) \in D$, then we can more reasonably assume that $a,b$ are close than $a,c$ being close. This leads us to the following method of defining
a set of close points of $a$: Let $L_x = \{t \in T | t = (x, b,c), b,c \in D\}$ be the set of all available triplets where $a$ is in the left position, and equivalently $M_x$ and $R_x$ 
for set of triplets where $x$ is in the middle and right position, respectively. Then we define: $P_a := \{x \in D \mid \left|   L_a \cap M_x\right| < \left| L_a \cap R_x \right| \}$
which is the set of all points that are more often closer to $a$ than they are farther away. We refer to these bipartitions as \textit{majority cuts}. Again, they capture
some cluster structure by assigning points that are close together to the same bipartition.

%TODO: Examining the quality of the cuts

