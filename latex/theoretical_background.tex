\chapter{Theoretical Background}\label{theory}
% chapter is not capitalized here, as it is not a title: https://academia.stackexchange.com/questions/9454/capitalisation-of-section-and-chapter-in-a-ph-d-thesis
In this chapter we will a brief introduction to the theoretical concepts used in this work. 
In particular, we will give an in-depth explanation about tangles, triplet data, and state-of-the-art methods of clustering triplet data.
% This explanation should suffice to understand the rest of this work.

\section{Tangles}\label{theory:tangles}
% What are Tangles? How are they used? What advantages do they
% prove?
Tangles have been used in mathematical graph theory to study highly cohesive structures, 
introduced originally by \cite{robertsonGraphMinorsObstructions1991}. 
Recently, interesting areas of application have been proposed:
\cite{diestelTanglesMonaLisa2017} makes a proof of concept that tangles could 
be used in image analysis.
\cite{diestelTanglesSocialSciences2019} describes how tangles can be used in social sciences, for 
example to identify different mindsets in people answering questionnaires.
\citep{fluckTanglesSingleLinkage2019} shows that, under specific circumstances, tangles can be
used to reconstruct the dendrogram in a hierarchical clustering setting. 

In recent times, \cite{klepperClusteringTanglesAlgorithmic2021} describes a very flexible setup,
where tangles are successfully applied to solve problems of clustering. 
The mentioned work develops an algorithmic framework and gives theoretical guarantees for 
basic problem settings.
Additionally, it introduces simplified notations, adapted to the domain of computer science. 
When talking about tangles, we will use the definitions introduced therein. 

In this section, we now give a very brief introduction to the basic notions, theory and applications of tangles in a clustering context.
For in-depth explanations of the algorithms and exact procedures, refer to 
\cite{klepperClusteringTanglesAlgorithmic2021}.


\subsection{What is a Tangle?}
The central object in tangles theory is a \textit{bipartition}. 
A bipartition is simply a way of dividing a set of elements $V =  \{ v_1, v_2, \ldots \}$ into two distinct subsets $A, B \subset V$, such that
$A \cap B = \O$ and $A \cup B = V$. We can also write a bipartition as $P = \{A, \overline{A}\}$, with $A \subset V$ and $\overline{A}$ being the
complement of $A$ with respect to $V$. For such a bipartition to be useful in clustering, we expect it to hold some degree of information about the cluster 
structure of our data. This means that a good bipartition should not separate groups of data that are tightly coupled.
On a graph, a good bipartition $P = \{A, \overline{A}\}$ might separate the set of nodes $V$ such that there 
are only a few edges between $A$ and $\overline{A}$. How useful a bipartition is will be quantified by a \textit{cost function} $c: \mathcal{P}(V) \to \R$, 
with $\mathcal{P}(V)$ denoting the power set of V. One is free to choose this cost function and the performance of tangles will also depend on
how well the cost function and the problem setting fit together. 

Assume that for a set of elements $V$ we have a set of bipartitions $\mathcal{B} = \{\{A_1, \overline{A_1}\}, \ldots, \{A_n, \overline{A_n}\} \} $ on $V$.
This set of bipartitions, together with the cost function, should tell us a lot about the cluster structure of the data:
for all bipartitions, we know how much they do or don't separate dense regions in $V$. This information in the bipartitions is aggregated and brought into a useable form by the tangles framework. 
For this, we process $\mathcal{B}$ to a set of so-called \textit{tangles}. These
correspond to specific ways of orienting the cuts such that they point to cohesive structures in the data. 
Orienting here means that we pick one specific side of a bipartition. An \textit{Orientation} of $\mathcal{B}$ is then a set $O = \{o_1, o_2, \ldots o_n\}$, where $o_i$ 
corresponds to either the partition $A_i$ (oriented \textit{left}) or $\overline{A_i}$ (oriented \textit{right}). A consistent orientation (which we then call a \textit{tangle}) is an orientation for which:
\begin{align}
    \forall A,B,C \in O: \left| A \cap B \cap C \right| \ge a
\end{align}
for some fixed parameter $a \in \N$, which we refer to as \textit{agreement} parameter. 

As is, a lot of sets of bipartitions wouldn't allow for any tangles, as there are too many cuts to consistently orient them. 
Imagine that our set of bipartitions would contain a few random cuts. 
On average, each of these halve our set of points, so we can at most orient on the order of $O(\log(n))$ many random bipartitions consistently.
This can be resolved using the cost function: we can simply restrict our tangles to a set of low-cost (and thus very insightful) cuts $P_{\psi}$, using a threshhold $\psi \in \R$ such that
$P_{\psi} = \{ c(P) \le \psi \}$. A tangle on $P_{\psi}$ is then said to have order $\psi$. We have included a schematic drawing of a tangle on a simple data set in \autoref{fig:tangles-example}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tangles-example.pdf}
    \caption{An example of how a simple tangle might look like if we assume a reasonably sized agreement (say $a = 3$). 
        The red lines represent cuts which divide the sets of points into a bipartition.  
        The tangle that is depicted here orients all bipartitions to the left side (indicated by the arrows), 
        so that they point to the dense structure on the left. 
        Another possible tangle might orient all cuts to the right. Notice that a tangle on this set of bipartitions can only either orient all bipartitions to the left or to the right, else
        the consistency criterion is violated. This might already give a good intuition on why tangles are able to find dense structures in data.}
    \label{fig:tangles-example}
\end{figure}

\subsection{Processing tangles to a clustering}
As we have seen in \autoref{fig:tangles-example}, a tangle might correspond directly to a cluster. But, a set of bipartitions usually allows for a wide 
variety of possible tangles, some of them pointing to different or overlapping clusters. We now have to process this set of tangles into a useable set of clusters.
This step is a bit involved and we aim to only give a rough, intuitional overview here. 

Given a set of bipartitions $\mathcal{B} = \{b_1, b_2, \ldots b_n\} $, we first want to determine for all possible orders $\psi$ the sets of all tangles of order $\psi$ on $\mathcal{B}$ 
according to a given cost function $c$. Intuitively, this order on the set of tangles determines how coarse the clustering is they define.
If we only use bipartitions with a low cost (so in the case of small $\psi$), then the bipartitions cut only through very loosely connected structures. 
If the cost is higher, the bipartitions are allowed to cut through more dense regions. This directly induces a sort of hierarchy, where we go from coarse cluster
structures to fine ones with increasing $\psi$.

To aggregate the tangles of varying costs, we build a tree structure on the set of tangles, called the \textit{tangle search tree}. In the tangle search tree, 
each node represents a tangle. Every level of the tree contains all possible tangles of a certain threshhold $\psi_i$ which directly corresponds to the cost of bipartition $b_i$. 
The exact contents of the tangle corresponding to a particular node is determined by walking the path from the root to the node, adding bipartition $b_i$ to the tangle in a left-oriented way, if it is a left child and in a right-oriented way if it is a right child. An exemplary tangle search tree is illustrated in \autoref{fig:tangles-tree-example}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tangles-tree-example.png}
    \caption{An example of a possible tangles search tree for a set of bipartitions $\mathcal{B} = \{\{A_1, \overline{A_1}\}, \{A_2, \overline{A_2}\}, \{A_3, \overline{A_3}\} \}$. 
        Each level corresponds to the tangles of the order given by the bipartition $P_i$ that is written on the dashed line to the left of it.
        As an example, if we would now want to determine
        the content of the node in level 3, we just walk from the root to it and add the bipartitions in the direction indicated by the tree. 
        We end up with $T = \{\overline{A_1}, \overline{A_2}, A_3\}$. 
        Figure taken with permission from \cite{klepperClusteringTanglesAlgorithmic2021}.}
    \label{fig:tangles-tree-example}
\end{figure}

From this tangle search tree, we can now obtain a soft, hierarchical clustering. \textit{Soft} means that every data point is assigned
a probability of belonging to each cluster. \textit{Hierarchical} means that we have a hierarchy on the resulting clusters, which arises naturally
from the form of the tangles search tree. For the clustering, the interesting nodes are those
where the tree splits up (as this represents a new clustering) and the leaves (which correspond to the clusters). 
For each of these \textit{splitting nodes}, we determine the set of \textit{characterizing cuts}.
A cut belongs to this set, if it is both oriented the same way inside the subtrees, and oriented in a different way between the two subtrees.
To illustrate this, we take a look at the exemplary tree in \autoref{fig:tangles-tree-example}. Here, for the root node, $P_1$ is a characterizing cut (pretty trivially),
while $P_2$ is not: below the node $A_1$, the bipartition is both oriented to the left and to to the right, violating the requirement that the cuts are always oriented
the same way inside the subtrees. 

To get a soft clustering, we can leverage the characterising cuts, which express some kind of consensus on how to align the bipartitions in the subtrees of the node. 
For a soft clustering, we have to determine with what probability a point $a$ belongs to a cluster $C$. 
To do this, we start from the root. At every splitting node of the tree (which includes the root), we now examine the set of characterising cuts.
We then orient them in the same way they are oriented in the left subtree and count how many of these so oriented cuts contain $a$. Divided by the total amount of characterising cuts at the
splitting node, we receive the probability $p_L$ that $a$ belongs to the left cluster complex. As the characterising cuts are always oriented differently in the two subtrees, the probability
of $a$ belonging to the right cluster complex is then given by $1 - p_L$. We can include these probabilites on the edges of our tree. 
To find out with what probability $a$ belongs to $C$, we
walk down the from the root to the leaf node that corresponds to $C$ and simply take the product of all edge probabilites that we encountered along the way.
By assigning each node to the cluster it belongs to with the highest probability, we obtain the corresponding \textit{hard} clustering.
    
% Given a set of bipartitions $\mathcal{B}$, we first sort the bipartitions according to their costs given by the cost function $c$. 
% We now iterate over the set of bipartitions, while building up a binary tree in a level-wise fashion. For every bipartition $P_i$, 
% we add it to each node in level $i-1$, if it is consistent with the tangle inducded by the given node. 

\section{Ordinal Constraints}
A dataset of ordinal constraints consists of object for which we don't know their exact attributes or features, but only how they relate to other objects
in the form of comparisons such as \textit{item $i$ is closer to item $j$ than to item $k$}. Formally, we assume that $i, j, k$ are from 
a set $D$ where we can define a dissimilarity function $d: D \times D \to \R$. Note that $d$ can be a metric on $D$, but does not have to be.
Using d, we can express our above constraint as $d(i, j) < d(i, k)$. Such data often appears when humans are asked to judge objects, as they often naturally perform better on comparing objects
than on accurately placing them on an abstract scale \citep{stewartAbsoluteIdentificationRelative2005}. Applications consist of estimating perceptual scales in psychophysics 
\citep{haghiriEstimationPerceptualScales2020} or crowd-sourcing clustering algorithms \citep{ukkonenCrowdsourcedCorrelationClustering2017}. 
We want to keep our focus here mainly the realm of psychophysics.


\subsection{Forms of Ordinal Constraints}
Ordinal constraints take one of two possible forms: quadruplets (used for example in \cite{ghoshdastidarFoundationsComparisonBasedHierarchical2019}) and triplets (used for example in \cite{vankadaraInsightsOrdinalEmbedding2021,haghiriComparisonBasedFrameworkPsychophysics2019}).
A quadruplet is a quadruple (a,b,c,d) 
and expresses the following constraint on our data points: $\text{d}(a,b) < \text{d}(c,d)$. 
Analogously, a triplet is a triple (a,b,c), which expresses $\text{d}(a,b) < \text{d}(a,c)$. 
A triplet (a,b,c) can also by expressed by the quadruplet (a,b,a,c), making quadruplets strictly more general. 

Datasets of triplet comparisons (which we also simply call triplet data) are almost always obtained by asking humans participants. For example, we might 
collect triplet data on images by presenting human participants with three images $a,b,c$, 
with image $a$ as anchor point and ask them, \textit{is $b$ or $c$ closer to $a$?}.
The way that participants are questioned varies on the context of the experiment (and might also influence their answers). An
example is presenting the participant with three images, and asking \textit{which is the most central image?}, or \textit{which is the odd one out?},
but the results can then always transformed back to triplet format for further prcoessing. For example, if $a$ is the \textit{odd-one-out} of the three elements $a,b,c$, 
then we know that $d(b,c) < d(b,a)$ and $d(c, b) < d(c,a)$. 

\subsection{Ordinal embeddings}
One of the most central problems when dealing with triplet data consists of finding a so called \textit{ordinal embedding} of the data. If we have a set of triplet comparisons $T = \{t_1, t_2, \ldots t_n\}$, 
of the form $t_i = \left( a,b,c \right)$, encoding that $d(a,b) < d(a,c)$, 
we want to find a set of points $y_1, y_2, \ldots y_n \in \R^{m}$, such that they uphold most of the original triplet constraints in $\R^{m}$ with the euclidean distance
as metric. More formally, we want to minimize \citep{vankadaraInsightsOrdinalEmbedding2021}
\begin{align*}
    \min_{y_1, \ldots y_n \in \R^{m}} \sum_{t=\left( i,j,k \right)  \in T} \mathbbm{1}_{ \|y_i - y_j\|_2 < \|y_i - y_k\|_2 }
.\end{align*}
This problem is difficult to optimize and thus various algorithms have been proposed that solve a relaxed or modified version of this objective function. 
These include Soft Ordinal Embedding  (SOE, \cite{teradaLocalOrdinalEmbedding2014}), Generalized Non-Metric Multidimensional Scaling (GNDMS, \cite{agarwalGeneralizedNonmetricMultidimensional2007}), 
Crowd Kernel Learning (CKL, \cite{tamuzAdaptivelyLearningCrowd2011}), Fast Ordinal Triplets Embedding (FORTE, \cite{jainFiniteSamplePrediction2016}),
T-Stochatic Triplet Embedding (T-STE, \cite{laurensvandermaatenStochasticTripletEmbedding2012}) and various others. 

It has been proven that if the original points come from the space $\R^{m}$, one can recover the points (up to scaling and orthogonal transformations) with $O(mn\log(n))$ many triplet comparisons
\citep{jainFiniteSamplePrediction2016}. On this basis, an ordinal embedding can be used together with more classical machine learning algorithms, such as support vector machines or k-Means,
for other machine learning tasks. Examples tasks are classification \citep{tamuzAdaptivelyLearningCrowd2011, kleindessnerLensDepthFunction2017} or clustering \citep{kleindessnerLensDepthFunction2017}.

