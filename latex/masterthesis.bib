
@article{adlamRandomMatrixPerspective2021,
  title = {A {{Random Matrix Perspective}} on {{Mixtures}} of {{Nonlinearities}} for {{Deep Learning}}},
  author = {Adlam, Ben and Levinson, Jake and Pennington, Jeffrey},
  year = {2021},
  month = nov,
  journal = {arXiv:1912.00827 [cs, stat]},
  eprint = {1912.00827},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of random feature regression with features \$F=f(WX+B)\$ for a random weight matrix \$W\$ and random bias vector \$B\$, obtaining exact formulae for the asymptotic training and test errors for data generated by a linear teacher model. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we find that a mixture of nonlinearities can improve both the training and test errors over the best single nonlinearity, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/46X8I7CR/Adlam et al. - 2021 - A Random Matrix Perspective on Mixtures of Nonline.pdf;/Users/almico/Zotero/storage/RS28N9JZ/1912.html}
}

@article{agarwalGeneralizedNonmetricMultidimensional,
  title = {Generalized {{Non-metric Multidimensional Scaling}}},
  author = {Agarwal, Sameer and Wills, Josh and Cayton, Lawrence and Lanckriet, Gert and Kriegman, David and Belongie, Serge},
  pages = {8},
  abstract = {We consider the non-metric multidimensional scaling problem: given a set of dissimilarities {$\increment$}, find an embedding whose inter-point Euclidean distances have the same ordering as {$\increment$}. In this paper, we look at a generalization of this problem in which only a set of order relations of the form dij {$<$} dkl are provided. Unlike the original problem, these order relations can be contradictory and need not be specified for all pairs of dissimilarities. We argue that this setting is more natural in some experimental settings and propose an algorithm based on convex optimization techniques to solve this problem. We apply this algorithm to human subject data from a psychophysics experiment concerning how reflectance properties are perceived. We also look at the standard NMDS problem, where a dissimilarity matrix {$\increment$} is provided as input, and show that we can always find an orderrespecting embedding of {$\increment$}.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/YTKK3EDY/Agarwal et al. - Generalized Non-metric Multidimensional Scaling.pdf}
}

@inproceedings{bellSecureSingleServerAggregation2020,
  title = {Secure {{Single-Server Aggregation}} with ({{Poly}}){{Logarithmic Overhead}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Bell, James Henry and Bonawitz, Kallista A. and Gasc{\'o}n, Adri{\`a} and Lepoint, Tancr{\`e}de and Raykova, Mariana},
  year = {2020},
  month = oct,
  pages = {1253--1269},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3372297.3417885},
  abstract = {Secure aggregation is a cryptographic primitive that enables a server to learn the sum of the vector inputs of many clients. Bonawitz et al. (CCS 2017) presented a construction that incurs computation and communication for each client linear in the number of parties. While this functionality enables a broad range of privacy preserving computational tasks, scaling concerns limit its scope of use. We present the first constructions for secure aggregation that achieve polylogarithmic communication and computation per client. Our constructions provide security in the semi-honest and the semimalicious settings where the adversary controls the server and a {$\gamma$} -fraction of the clients, and correctness with up to {$\delta$} -fraction dropouts among the clients. Our constructions show how to replace the complete communication graph of Bonawitz et al., which entails the linear overheads, with a k-regular graph of logarithmic degree while maintaining the security guarantees.},
  isbn = {978-1-4503-7089-9},
  langid = {english},
  note = {Note how annoying the introduction is, as they don't tell us at the beginning what they do, but start off with all of the related literature for 5 paragraphs.},
  file = {/Users/almico/Zotero/storage/C6NRJNGZ/Bell et al. - 2020 - Secure Single-Server Aggregation with (Poly)Logari.pdf}
}

@misc{COLT2021Theory,
  title = {{{COLT}} 2021: {{A Theory}} of {{Heuristic Learnability}}},
  howpublished = {http://learningtheory.org/colt2021/virtual/poster\_37.html},
  file = {/home/almico/Downloads/nanashima21a.pdf;/Users/almico/Zotero/storage/MDPPAE58/poster_37.html}
}

@article{dasguptaExplainableMeansMedians2020,
  title = {Explainable \$k\$-{{Means}} and \$k\$-{{Medians Clustering}}},
  author = {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal and Rashtchian, Cyrus},
  year = {2020},
  month = sep,
  journal = {arXiv:2002.12538 [cs, stat]},
  eprint = {2002.12538},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Clustering is a popular form of unsupervised learning for geometric data. Unfortunately, many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the \$k\$-means and \$k\$-medians objectives: Must there exist a tree-induced clustering whose cost is comparable to that of the best unconstrained clustering, and if so, how can it be found? In terms of negative results, we show, first, that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and second, that any tree-induced clustering must in general incur an \$\textbackslash Omega(\textbackslash log k)\$ approximation factor compared to the optimal clustering. On the positive side, we design an efficient algorithm that produces explainable clusters using a tree with \$k\$ leaves. For two means/medians, we show that a single threshold cut suffices to achieve a constant factor approximation, and we give nearly-matching lower bounds. For general \$k \textbackslash geq 2\$, our algorithm is an \$O(k)\$ approximation to the optimal \$k\$-medians and an \$O(k\^2)\$ approximation to the optimal \$k\$-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/DPJNPCEH/Dasgupta et al. - 2020 - Explainable $k$-Means and $k$-Medians Clustering.pdf;/Users/almico/Zotero/storage/ADHFQ4VC/2002.html}
}

@article{diestelAbstractSeparationSystems2018,
  title = {Abstract {{Separation Systems}}},
  author = {Diestel, Reinhard},
  year = {2018},
  month = mar,
  journal = {Order},
  volume = {35},
  number = {1},
  eprint = {1406.3797},
  eprinttype = {arxiv},
  pages = {157--170},
  issn = {0167-8094, 1572-9273},
  doi = {10.1007/s11083-017-9424-5},
  abstract = {Abstract separation systems provide a simple general framework in which both tree-shape and high cohesion of many combinatorial structures can be expressed, and their duality proved. Applications range from tangle-type duality and tree structure theorems in graphs, matroids or CW-complexes to, potentially, image segmentation and cluster analysis. This paper is intended as a concise common reference for the basic definitions and facts about abstract separation systems in these and any future papers using this framework.},
  archiveprefix = {arXiv},
  keywords = {06-XX; 05C05; 05C83,Mathematics - Combinatorics},
  note = {Comment: This is Section 2, considerably expanded, of the predecessor version 4 of this thread. Sections 3-4 on abstract duality have migrated to [8], Section 5 on applications has moved to [9]},
  file = {/Users/almico/Zotero/storage/6I2IBK54/Diestel - 2018 - Abstract Separation Systems.pdf;/Users/almico/Zotero/storage/Q79H25G2/1406.html}
}

@article{fraimanInterpretableClusteringUsing2011,
  title = {Interpretable {{Clustering}} Using {{Unsupervised Binary Trees}}},
  author = {Fraiman, Ricardo and Ghattas, Badih and Svarc, Marcela},
  year = {2011},
  month = mar,
  abstract = {We herein introduce a new method of interpretable clustering that uses unsupervised binary trees. It is a three-stage procedure, the first stage of which entails a series of recursive binary splits to reduce the heterogeneity of the data within the new subsamples. During the second stage (pruning), consideration is given to whether adjacent nodes can be aggregated. Finally, during the third stage (joining), similar clusters are joined together, even if they do not descend from the same node originally. Consistency results are obtained, and the procedure is used on simulated and real data sets.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/3C67GWVD/Fraiman et al. - 2011 - Interpretable Clustering using Unsupervised Binary.pdf}
}

@article{ghoshdastidarFoundationsComparisonBasedHierarchical2019,
  title = {Foundations of {{Comparison-Based Hierarchical Clustering}}},
  author = {Ghoshdastidar, Debarghya and Perrot, Micha{\"e}l and {von Luxburg}, Ulrike},
  year = {2019},
  month = jun,
  journal = {arXiv:1811.00928 [cs, stat]},
  eprint = {1811.00928},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form "objects \$i\$ and \$j\$ are more similar than objects \$k\$ and \$l\$." Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 26 pages},
  file = {/Users/almico/Zotero/storage/C77UXXLN/Ghoshdastidar et al. - 2019 - Foundations of Comparison-Based Hierarchical Clust.pdf;/Users/almico/Zotero/storage/DDJV2Y3U/1811.html}
}

@article{ghoshLandmarkOrdinalEmbedding2019,
  title = {Landmark {{Ordinal Embedding}}},
  author = {Ghosh, Nikhil and Chen, Yuxin and Yue, Yisong},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.12379 [cs, stat]},
  eprint = {1910.12379},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we aim to learn a low-dimensional Euclidean representation from a set of constraints of the form "item j is closer to item i than item k". Existing approaches for this "ordinal embedding" problem require expensive optimization procedures, which cannot scale to handle increasingly larger datasets. To address this issue, we propose a landmark-based strategy, which we call Landmark Ordinal Embedding (LOE). Our approach trades off statistical efficiency for computational efficiency by exploiting the low-dimensionality of the latent embedding. We derive bounds establishing the statistical consistency of LOE under the popular Bradley-Terry-Luce noise model. Through a rigorous analysis of the computational complexity, we show that LOE is significantly more efficient than conventional ordinal embedding approaches as the number of items grows. We validate these characterizations empirically on both synthetic and real datasets. We also present a practical approach that achieves the "best of both worlds", by using LOE to warm-start existing methods that are more statistically efficient but computationally expensive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: NeurIPS 2019},
  file = {/Users/almico/Zotero/storage/CDRTQ63S/Ghosh et al. - 2019 - Landmark Ordinal Embedding.pdf;/Users/almico/Zotero/storage/DA8Z8BGG/1910.html}
}

@article{haghiriComparisonBasedFrameworkPsychophysics2019,
  title = {Comparison-{{Based Framework}} for {{Psychophysics}}: {{Lab}} versus {{Crowdsourcing}}},
  shorttitle = {Comparison-{{Based Framework}} for {{Psychophysics}}},
  author = {Haghiri, Siavash and Rubisch, Patricia and Geirhos, Robert and Wichmann, Felix and {von Luxburg}, Ulrike},
  year = {2019},
  month = jul,
  journal = {arXiv:1905.07234 [cs, stat]},
  eprint = {1905.07234},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Traditionally, psychophysical experiments are conducted by repeated measurements on a few well-trained participants under well-controlled conditions, often resulting in, if done properly, high quality data. In recent years, however, crowdsourcing platforms are becoming increasingly popular means of data collection, measuring many participants at the potential cost of obtaining data of worse quality. In this paper we study whether the use of comparison-based (ordinal) data, combined with machine learning algorithms, can boost the reliability of crowdsourcing studies for psychophysics, such that they can achieve performance close to a lab experiment. To this end, we compare three setups: simulations, a psychophysics lab experiment, and the same experiment on Amazon Mechanical Turk. All these experiments are conducted in a comparison-based setting where participants have to answer triplet questions of the form "is object x closer to y or to z?". We then use machine learning to solve the triplet prediction problem: given a subset of triplet questions with corresponding answers, we predict the answer to the remaining questions. Considering the limitations and noise on MTurk, we find that the accuracy of triplet prediction is surprisingly close---but not equal---to our lab study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/E4VRVIEV/Haghiri et al. - 2019 - Comparison-Based Framework for Psychophysics Lab .pdf;/Users/almico/Zotero/storage/UR4PXJ4W/1905.html}
}

@inproceedings{haghiriComparisonBasedNearestNeighbor2017,
  title = {Comparison-{{Based Nearest Neighbor Search}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Haghiri, Siavash and Ghoshdastidar, Debarghya and von Luxburg, Ulrike},
  year = {2017},
  month = apr,
  pages = {851--859},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points i and j is smaller than the distance between the points i and k. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure  probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/KTBYEM9S/Haghiri et al. - 2017 - Comparison-Based Nearest Neighbor Search.pdf}
}

@inproceedings{haghiriComparisonBasedRandomForests2018,
  title = {Comparison-{{Based Random Forests}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Haghiri, Siavash and Garreau, Damien and Luxburg, Ulrike},
  year = {2018},
  month = jul,
  pages = {1871--1880},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Assume we are given a set of items from a general metric space, but we neither have access to the representation of the data nor to the distances between data points. Instead, suppose that we can actively choose a triplet of items (A, B, C) and ask an oracle whether item A is closer to item B or to item C. In this paper, we propose a novel random forest algorithm for regression and classification that relies only on such triplet comparisons. In the theory part of this paper, we establish sufficient conditions for the consistency of such a forest. In a set of comprehensive experiments, we then demonstrate that the proposed random forest is efficient both for classification and regression. In particular, it is even competitive with other methods that have direct access to the metric representation of the data.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/W7TKNAGZ/Haghiri et al. - 2018 - Comparison-Based Random Forests.pdf;/Users/almico/Zotero/storage/YS6HYFJA/Haghiri et al. - 2018 - Comparison-Based Random Forests.pdf}
}

@article{haghiriEstimationPerceptualScales2019,
  title = {Estimation of Perceptual Scales Using Ordinal Embedding},
  author = {Haghiri, Siavash and Wichmann, Felix and {von Luxburg}, Ulrike},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.07962 [cs, stat]},
  eprint = {1908.07962},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper we address the problem of measuring and analysing sensation, the subjective magnitude of one's experience. We do this in the context of the method of triads: the sensation of the stimulus is evaluated via relative judgments of the form: ``Is stimulus Si more similar to stimulus Sj or to stimulus Sk?''. We propose to use ordinal embedding methods from machine learning to estimate the scaling function from the relative judgments. We review two relevant and well-known methods in psychophysics which are partially applicable in our setting: non-metric multi-dimensional scaling (NMDS) and the method of maximum likelihood difference scaling (MLDS). We perform an extensive set of simulations, considering various scaling functions, to demonstrate the performance of the ordinal embedding methods. We show that in contrast to existing approaches our ordinal embedding approach allows, first, to obtain reasonable scaling function from comparatively few relative judgments, second, the estimation of non-monotonous scaling functions, and, third, multi-dimensional perceptual scales. In addition to the simulations, we analyse data from two real psychophysics experiments using ordinal embedding methods. Our results show that in the one-dimensional, monotonically increasing perceptual scale our ordinal embedding approach works as well as MLDS, while in higher dimensions, only our ordinal embedding methods can produce a desirable scaling function. To make our methods widely accessible, we provide an R-implementation and general rules of thumb on how to use ordinal embedding in the context of psychophysics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/TPIZSI9B/Haghiri et al. - 2019 - Estimation of perceptual scales using ordinal embe.pdf}
}

@inproceedings{huangVariationalPerspectiveDiffusionBased2021a,
  title = {A {{Variational Perspective}} on {{Diffusion-Based Generative Models}} and {{Score Matching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron},
  year = {2021},
  month = may,
  abstract = {We derived an ELBO for continuous-time diffusion models using stochastic calculus, and made connection to continuous-time normalizing flows, hierarchical VAE, and score-based generative models.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/V8EN8NJ2/Huang et al. - 2021 - A Variational Perspective on Diffusion-Based Gener.pdf;/Users/almico/Zotero/storage/6FPZ4SU9/forum.html}
}

@article{kleindessnerLensDepthFunction2016,
  title = {Lens Depth Function and K-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis},
  shorttitle = {Lens Depth Function and K-Relative Neighborhood Graph},
  author = {Kleindessner, Matth{\"a}us and {von Luxburg}, Ulrike},
  year = {2016},
  month = feb,
  abstract = {In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as \$d(A,B)},
  langid = {english},
  file = {/Users/almico/Zotero/storage/4PYB3CPZ/Kleindessner and von Luxburg - 2016 - Lens depth function and k-relative neighborhood gr.pdf;/Users/almico/Zotero/storage/ZJLQ8PFS/1602.html}
}

@article{klepperClusteringTanglesAlgorithmic2020,
  title = {Clustering with {{Tangles}}: {{Algorithmic Framework}} and {{Theoretical Guarantees}}},
  shorttitle = {Clustering with {{Tangles}}},
  author = {Klepper, Solveig and Elbracht, Christian and Fioravanti, Diego and Kneip, Jakob and Rendsburg, Luca and Teegen, Maximilian and {von Luxburg}, Ulrike},
  year = {2020},
  month = jun,
  abstract = {Originally, tangles had been invented as an abstract tool in mathematical graph theory to prove the famous graph minor theorem. In this paper we showcase the practical potential of tangles in machine learning applications. Given a collection of weak cuts (bipartitions) of any dataset, tangles provide a mechanism to aggregate these cuts such that they point in the direction of a dense structure. As a result, a cluster is softly characterized by a set of consistent pointers. This highly flexible approach can solve soft clustering problems in a variety of setups, ranging from questionnaires over community detection in graphs to clustering points in metric spaces. The output of our proposed framework is of a hierarchical nature and induces the notion of a soft dendrogram, which can be helpful to explore the cluster structure of a dataset. The computational complexity of aggregating the cuts is only linear in the number of data points and places the bottleneck on generating the cuts, for which simple and fast algorithms form a sufficient basis. The contribution of this paper is to translate the abstract mathematical literature into algorithms that can be used in machine learning. We demonstrate the power of tangles in many different use cases and provide theoretical guarantees for their performance.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/BD9NTYAT/Klepper et al. - 2020 - Clustering with Tangles Algorithmic Framework and.pdf;/Users/almico/Zotero/storage/TH2IQSKE/2006.html}
}

@inproceedings{liWhyAreConvolutional2020,
  title = {Why {{Are Convolutional Nets More Sample-Efficient}} than {{Fully-Connected Nets}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
  year = {2020},
  month = sep,
  abstract = {Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of...},
  langid = {english},
  file = {/Users/almico/Zotero/storage/K7L5YVIH/Li et al. - 2020 - Why Are Convolutional Nets More Sample-Efficient t.pdf;/Users/almico/Zotero/storage/WE2J2CB6/forum.html}
}

@article{luxburgClusteringStabilityOverview2010,
  title = {Clustering {{Stability}}: {{An Overview}}},
  shorttitle = {Clustering {{Stability}}},
  author = {von Luxburg, Ulrike},
  year = {2010},
  month = apr,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {2},
  number = {3},
  pages = {235--274},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000008},
  abstract = {Clustering Stability: An Overview},
  langid = {english},
  file = {/Users/almico/Zotero/storage/KBX5X5NT/goldenberg2009.pdf;/Users/almico/Zotero/storage/SGSCKLJM/MAL-008.html}
}

@article{moshkovitzConnectingInterpretabilityRobustness2021,
  title = {Connecting {{Interpretability}} and {{Robustness}} in {{Decision Trees}} through {{Separation}}},
  author = {Moshkovitz, Michal and Yang, Yao-Yuan and Chaudhuri, Kamalika},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.07048 [cs, stat]},
  eprint = {2102.07048},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent research has recognized interpretability and robustness as essential properties of trustworthy classification. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Specifically, we focus on interpretation using decision trees and robustness to \$l\_\{\textbackslash infty\}\$-perturbation. Previous works defined the notion of \$r\$-separation as a sufficient condition for robustness. We prove upper and lower bounds on the tree size in case the data is \$r\$-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the first algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments confirm that our algorithm yields classifiers that are both interpretable and robust and have high accuracy. The code for the experiments is available at https://github.com/yangarbiter/interpretable-robust-trees .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/BQQIDRNF/Moshkovitz et al. - 2021 - Connecting Interpretability and Robustness in Deci.pdf;/Users/almico/Zotero/storage/DGUYE4UV/2102.html}
}

@article{robertsonGraphMinorsObstructions1991,
  title = {Graph Minors. {{X}}. {{Obstructions}} to Tree-Decomposition},
  author = {Robertson, Neil and Seymour, P. D},
  year = {1991},
  month = jul,
  journal = {Journal of Combinatorial Theory, Series B},
  volume = {52},
  number = {2},
  pages = {153--190},
  issn = {0095-8956},
  doi = {10.1016/0095-8956(91)90061-N},
  abstract = {Roughly, a graph has small ``tree-width'' if it can be constructed by piecing small graphs together in a tree structure. Here we study the obstructions to the existence of such a tree structure. We find, for instance: 1.(i) a minimax formula relating tree-width with the largest such obstructions2.(ii) an association between such obstructions and large grid minors of the graph3.(iii) a ``tree-decomposition'' of the graph into pieces corresponding with the obstructions. These results will be of use in later papers.},
  langid = {english},
  note = {The paper that is cited in the tangles paper as basis for tangles.},
  file = {/Users/almico/Zotero/storage/TJIXLF3X/Robertson and Seymour - 1991 - Graph minors. X. Obstructions to tree-decompositio.pdf;/Users/almico/Zotero/storage/SZVF968X/009589569190061N.html}
}

@inproceedings{robinsonContrastiveLearningHard2020,
  title = {Contrastive {{Learning}} with {{Hard Negative Samples}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Robinson, Joshua David and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
  year = {2020},
  month = sep,
  abstract = {We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard...},
  langid = {english},
  file = {/Users/almico/Zotero/storage/IK9HINQC/Robinson et al. - 2020 - Contrastive Learning with Hard Negative Samples.pdf;/Users/almico/Zotero/storage/A8JTTJZJ/forum.html}
}

@article{rudinStopExplainingBlack2018a,
  title = {Stop {{Explaining Black Box Machine Learning Models}} for {{High Stakes Decisions}} and {{Use Interpretable Models Instead}}},
  author = {Rudin, Cynthia},
  year = {2018},
  month = nov,
  abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to \textbackslash textit\{explain\} black box models, rather than creating models that are \textbackslash textit\{interpretable\} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/IEPIZ8DS/Rudin - 2018 - Stop Explaining Black Box Machine Learning Models .pdf;/Users/almico/Zotero/storage/8YG7UTZR/1811.html}
}

@inproceedings{saisubramanianBalancingTradeoffClustering2020,
  title = {Balancing the {{Tradeoff Between Clustering Value}} and {{Interpretability}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Saisubramanian, Sandhya and Galhotra, Sainyam and Zilberstein, Shlomo},
  year = {2020},
  month = feb,
  series = {{{AIES}} '20},
  pages = {351--357},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3375627.3375843},
  abstract = {Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a {$\beta$}-interpretable clustering algorithm that ensures that at least {$\beta$} fraction of nodes in each cluster share the same feature value. The tunable parameter {$\beta$} is user-specified. We also present a more efficient algorithm for scenarios with {$\beta\backslash$}!=\textbackslash!1\$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.},
  isbn = {978-1-4503-7110-0},
  keywords = {centroid-based clustering,interpretability},
  file = {/Users/almico/Zotero/storage/Y94ADT82/Saisubramanian et al. - 2020 - Balancing the Tradeoff Between Clustering Value an.pdf}
}

@article{semenovaStudyRashomonCurves2021,
  title = {A Study in {{Rashomon}} Curves and Volumes: {{A}} New Perspective on Generalization and Model Simplicity in Machine Learning},
  shorttitle = {A Study in {{Rashomon}} Curves and Volumes},
  author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
  year = {2021},
  month = apr,
  journal = {arXiv:1908.01755 [cs, stat]},
  eprint = {1908.01755},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Rashomon effect occurs when many different explanations exist for the same phenomenon. In machine learning, Leo Breiman used this term to characterize problems where many accurate-but-different models exist to describe the same data. In this work, we study how the Rashomon effect can be useful for understanding the relationship between training and test performance, and the possibility that simple-yet-accurate models exist for many problems. We consider the Rashomon set - the set of almost-equally-accurate models for a given problem - and study its properties and the types of models it could contain. We present the Rashomon ratio as a new measure related to simplicity of model classes, which is the ratio of the volume of the set of accurate models to the volume of the hypothesis space; the Rashomon ratio is different from standard complexity measures from statistical learning theory. For a hierarchy of hypothesis spaces, the Rashomon ratio can help modelers to navigate the trade-off between simplicity and accuracy. In particular, we find empirically that a plot of empirical risk vs. Rashomon ratio forms a characteristic \$\textbackslash Gamma\$-shaped Rashomon curve, whose elbow seems to be a reliable model selection criterion. When the Rashomon set is large, models that are accurate - but that also have various other useful properties - can often be obtained. These models might obey various constraints such as interpretability, fairness, or monotonicity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Revisited sections 3, 4, 5, 6, 7, and 8},
  file = {/Users/almico/Zotero/storage/9ARSVZCD/Semenova et al. - 2021 - A study in Rashomon curves and volumes A new pers.pdf;/Users/almico/Zotero/storage/4L6Z52IW/1908.html}
}

@article{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  journal = {arXiv:2011.13456 [cs, stat]},
  eprint = {2011.13456},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\textbackslash aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICLR 2021 (Oral)},
  file = {/Users/almico/Zotero/storage/ZPCIDVIJ/2011.html}
}

@article{tamuzAdaptivelyLearningCrowd2011,
  title = {Adaptively {{Learning}} the {{Crowd Kernel}}},
  author = {Tamuz, Omer and Liu, Ce and Belongie, Serge and Shamir, Ohad and Kalai, Adam Tauman},
  year = {2011},
  month = jun,
  journal = {arXiv:1105.1033 [cs]},
  eprint = {1105.1033},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce an algorithm that, given n objects, learns a similarity matrix over all n\^2 pairs, from crowdsourced data alone. The algorithm samples responses to adaptively chosen triplet-based relative-similarity queries. Each query has the form "is object 'a' more similar to 'b' or to 'c'?" and is chosen to be maximally informative given the preceding responses. The output is an embedding of the objects into Euclidean space (like MDS); we refer to this as the "crowd kernel." SVMs reveal that the crowd kernel captures prominent and subtle features across a number of domains, such as "is striped" among neckties and "vowel vs. consonant" among letters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: 9 pages, 7 figures, Accepted to the 28th International Conference on Machine Learning (ICML), 2011},
  file = {/Users/almico/Zotero/storage/YJDHFUJP/Tamuz et al. - 2011 - Adaptively Learning the Crowd Kernel.pdf;/Users/almico/Zotero/storage/RMHAZLVJ/1105.html}
}

@article{teradaLocalOrdinalEmbedding,
  title = {Local {{Ordinal Embedding}}},
  author = {Terada, Yoshikazu},
  pages = {9},
  abstract = {We study the problem of ordinal embedding: given a set of ordinal constraints of the form distance(i, j) {$<$} distance(k, l) for some quadruples (i, j, k, l) of indices, the goal is to construct a point configuration x\textasciicircum 1, ..., x\textasciicircum n in Rp that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embedding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know ``local ordinal information'' in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/AJHG83H8/Terada - Local Ordinal Embedding.pdf}
}

@inproceedings{tripuraneniOverparameterizationImprovesRobustness2021,
  title = {Overparameterization {{Improves Robustness}} to {{Covariate Shift}} in {{High Dimensions}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Tripuraneni, Nilesh and Adlam, Ben and Pennington, Jeffrey},
  year = {2021},
  month = may,
  abstract = {We provide and analyze an exactly solvable model of random feature regression (with covariate shift) that reproduces existing empirical phenomena related to distribution shift.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/CNYFF9NX/Tripuraneni et al. - 2021 - Overparameterization Improves Robustness to Covari.pdf;/Users/almico/Zotero/storage/93ZSFG2X/forum.html}
}

@article{ukkonenCrowdsourcedCorrelationClustering2017,
  title = {Crowdsourced Correlation Clustering with Relative Distance Comparisons},
  author = {Ukkonen, Antti},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.08459 [cs]},
  eprint = {1709.08459},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Crowdsourced, or human computation based clustering algorithms usually rely on relative distance comparisons, as these are easier to elicit from human workers than absolute distance information. A relative distance comparison is a statement of the form "item A is closer to item B than to item C". However, many existing clustering algorithms that use relative distances are rather complex. They are often based on a two-step approach, where the relative distances are first used to learn either a distance matrix, or an embedding of the items, and then some standard clustering method is applied in a second step. In this paper we argue that it should be possible to compute a clustering directly from relative distance comparisons. Our ideas are built upon existing work on correlation clustering, a well-known non-parametric approach to clustering. The technical contribution of this work is twofold. We first define a novel variant of correlation clustering that is based on relative distance comparisons, and hence suitable for human computation. We go on to show that our new problem is closely related to basic correlation clustering, and use this property to design an approximation algorithm for our problem. As a second contribution, we propose a more practical algorithm, which we empirically compare against existing methods from literature. Experiments with synthetic data suggest that our approach can outperform more complex methods. Also, our method efficiently finds good and intuitive clusterings from real relative distance comparison data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  note = {Comment: short version published at IEEE ICDM 2017
\par
Other paper that deals with clustering directly
\par
Mentions that calculating an Embedding or a distance matrix from triplets before clustering seems wasted \textendash{} we agree! Tangles are better when we skip that step and can use triplets directly.
\par
NOTE: All attempts so far to use Tangles to cluster have relied on making a distance matrix beforehand (lenses, majority neighbours,\ldots ) and then using this to calculate cuts from them. Alternative is imputation on the triplets, which only works up to very small amounts of noise.},
  file = {/Users/almico/Zotero/storage/27JBPNHH/Ukkonen - 2017 - Crowdsourced correlation clustering with relative .pdf;/Users/almico/Zotero/storage/XI8M9PG6/1709.html}
}

@article{ullahMachineUnlearningAlgorithmic,
  title = {Machine {{Unlearning}} via {{Algorithmic Stability}}},
  author = {Ullah, Enayat and Mai, Tung and Rao, Anup B and Rossi, Ryan},
  pages = {17},
  abstract = {We study the problem of machine unlearning and identify a notion of algorithmic stability, Total Variation (TV) stability, which we argue, is suitable for the goal of exact unlearning. For convex risk minimization problems, we design TV-stable algorithms based on noisy Stochastic Gradient Descent (SGD). Our key contribution is the design of corresponding efficient unlearning algorithms, which are based on constructing a near-maximal coupling of Markov chains for the noisy SGD procedure. To understand the trade-offs between accuracy and unlearning efficiency, we give upper and lower bounds on excess empirical and populations risk of TV stable algorithms for convex risk minimization. Our techniques generalize to arbitrary non-convex functions, and our algorithms are differentially private as well.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/D8XVU3ZA/Ullah et al. - Machine Unlearning via Algorithmic Stability.pdf}
}

@inproceedings{vandermaatenStochasticTripletEmbedding2012,
  title = {Stochastic Triplet Embedding},
  booktitle = {2012 {{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}}},
  author = {{van der Maaten}, Laurens and Weinberger, Kilian},
  year = {2012},
  month = sep,
  pages = {1--6},
  issn = {2378-928X},
  doi = {10.1109/MLSP.2012.6349720},
  abstract = {This paper considers the problem of learning an embedding of data based on similarity triplets of the form ``A is more similar to B than to C''. This learning setting is of relevance to scenarios in which we wish to model human judgements on the similarity of objects. We argue that in order to obtain a truthful embedding of the underlying data, it is insufficient for the embedding to satisfy the constraints encoded by the similarity triplets. In particular, we introduce a new technique called t-Distributed Stochastic Triplet Embedding (t-STE) that collapses similar points and repels dissimilar points in the embedding - even when all triplet constraints are satisfied. Our experimental evaluation on three data sets shows that as a result, t-STE is much better than existing techniques at revealing the underlying data structure.},
  keywords = {Humans,Image color analysis,Kernel,Machine learning,Measurement,Optimization,Partial order embedding,similarity triplets,Stochastic processes},
  file = {/Users/almico/Zotero/storage/BFFCHCKL/van der Maaten and Weinberger - 2012 - Stochastic triplet embedding.pdf;/Users/almico/Zotero/storage/CGHIAFN5/6349720.html}
}

@article{vankadaraInsightsOrdinalEmbedding2019,
  title = {Insights into {{Ordinal Embedding Algorithms}}: {{A Systematic Evaluation}}},
  shorttitle = {Insights into {{Ordinal Embedding Algorithms}}},
  author = {Vankadara, Leena Chennuru and Haghiri, Siavash and Lohaus, Michael and Wahab, Faiz Ul and {von Luxburg}, Ulrike},
  year = {2019},
  month = dec,
  abstract = {The objective of ordinal embedding is to find a Euclidean representation of a set of abstract items, using only answers to triplet comparisons of the form "Is item \$i\$ closer to the item \$j\$ or item \$k\$?". In recent years, numerous algorithms have been proposed to solve this problem. However, there does not exist a fair and thorough assessment of these embedding methods and therefore several key questions remain unanswered: Which algorithms perform better when the embedding dimension is constrained or few triplet comparisons are available? Which ones scale better with increasing sample size or dimension? In our paper, we address these questions and provide the first comprehensive and systematic empirical evaluation of existing algorithms as well as a new neural network approach. We find that simple, relatively unknown, non-convex methods consistently outperform all other algorithms, including elaborate approaches based on neural networks or landmark approaches. This finding can be explained by our insight that many of the non-convex optimization approaches do not suffer from local optima. Our comprehensive assessment is enabled by our unified library of popular embedding algorithms that leverages GPU resources and allows for fast and accurate embeddings of millions of data points.},
  langid = {english},
  note = {Does not deal with Clustering at all.},
  file = {/Users/almico/Zotero/storage/YKMLDKMY/Vankadara et al. - 2019 - Insights into Ordinal Embedding Algorithms A Syst.pdf;/Users/almico/Zotero/storage/F6AHR38S/1912.html}
}


