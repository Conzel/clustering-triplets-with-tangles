
@inproceedings{agarwalGeneralizedNonmetricMultidimensional2007,
  title = {Generalized {{Non-metric Multidimensional Scaling}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Agarwal, Sameer and Wills, Josh and Cayton, Lawrence and Lanckriet, Gert and Kriegman, David and Belongie, Serge},
  year = {2007},
  month = mar,
  pages = {11--18},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We consider the non-metric multidimensional scaling problem: given a set of dissimilarities {$\Delta\Delta\backslash$}Delta, find an embedding whose inter-point Euclidean distances have the same ordering as {$\Delta\Delta\backslash$}Delta. In this paper, we look at a generalization of this problem in which only a set of order relations of the form {$\mathsl{d}\mathsl{i}\mathsl{j}<\mathsl{d}\mathsl{k}\mathsl{l}$}dij},
  langid = {english},
  file = {/Users/almico/Zotero/storage/QR9R3TYE/Agarwal et al. - 2007 - Generalized Non-metric Multidimensional Scaling.pdf}
}

@inproceedings{andertonScalingOrdinalEmbedding2019,
  title = {Scaling {{Up Ordinal Embedding}}: {{A Landmark Approach}}},
  shorttitle = {Scaling {{Up Ordinal Embedding}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Anderton, Jesse and Aslam, Javed},
  year = {2019},
  month = may,
  pages = {282--290},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Ordinal Embedding is the problem of placing n objects into R\^d to satisfy constraints like "object a is closer to b than to c." It can accommodate data that embeddings from features or distances cannot, but is a more difficult problem. We propose a novel landmark-based method as a partial solution. At small to medium scales, we present a novel combination of existing methods with some new theoretical justification. For very large values of n optimizing over an entire embedding breaks down, so we propose a novel method which first embeds a subset of m {$<<$} n objects and then embeds the remaining objects independently and in parallel. We prove a distance error bound for our method in terms of m and that it has O(dn log m) time complexity, and show empirically that it is able to produce high quality embeddings in a fraction of the time needed for any published method.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/QPV96VUC/Anderton and Aslam - 2019 - Scaling Up Ordinal Embedding A Landmark Approach.pdf;/Users/almico/Zotero/storage/TT47SJIR/Anderton and Aslam - 2019 - Scaling Up Ordinal Embedding A Landmark Approach.pdf}
}

@inproceedings{balakrishnanNoiseThresholdsSpectral2011,
  title = {Noise {{Thresholds}} for {{Spectral Clustering}}},
  booktitle = {Neural {{Information Processing Systems}} ({{NeurIPS}})},
  author = {Balakrishnan, Sivaraman and Xu, Min and Krishnamurthy, Akshay and Singh, Aarti},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We  additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the  clustering problem and compare the performance of spectral clustering to these  information theoretic limits. We also present experiments on simulated and real  world data illustrating our results.},
  file = {/Users/almico/Zotero/storage/EE8YUTXE/Balakrishnan et al. - 2011 - Noise Thresholds for Spectral Clustering.pdf}
}

@misc{dasguptaExplainableMeansMedians2020,
  title = {Explainable \$k\$-{{Means}} and \$k\$-{{Medians Clustering}}},
  author = {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal and Rashtchian, Cyrus},
  year = {2020},
  month = sep,
  number = {arXiv:2002.12538},
  eprint = {2002.12538},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2002.12538},
  abstract = {Clustering is a popular form of unsupervised learning for geometric data. Unfortunately, many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the \$k\$-means and \$k\$-medians objectives: Must there exist a tree-induced clustering whose cost is comparable to that of the best unconstrained clustering, and if so, how can it be found? In terms of negative results, we show, first, that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and second, that any tree-induced clustering must in general incur an \$\textbackslash Omega(\textbackslash log k)\$ approximation factor compared to the optimal clustering. On the positive side, we design an efficient algorithm that produces explainable clusters using a tree with \$k\$ leaves. For two means/medians, we show that a single threshold cut suffices to achieve a constant factor approximation, and we give nearly-matching lower bounds. For general \$k \textbackslash geq 2\$, our algorithm is an \$O(k)\$ approximation to the optimal \$k\$-medians and an \$O(k\^2)\$ approximation to the optimal \$k\$-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/IZDE7R6Z/Dasgupta et al. - 2020 - Explainable $k$-Means and $k$-Medians Clustering.pdf;/Users/almico/Zotero/storage/G8H9CXIT/2002.html}
}

@article{diestelAbstractSeparationSystems2018,
  title = {Abstract {{Separation Systems}}},
  author = {Diestel, Reinhard},
  year = {2018},
  month = mar,
  journal = {Order},
  volume = {35},
  number = {1},
  pages = {157--170},
  issn = {1572-9273},
  doi = {10.1007/s11083-017-9424-5},
  abstract = {Abstract separation systems provide a simple general framework in which both tree-shape and high cohesion of many combinatorial structures can be expressed, and their duality proved. Applications range from tangle-type duality and tree structure theorems in graphs, matroids or CW-complexes to, potentially, image segmentation and cluster analysis. This paper is intended as a concise common reference for the basic definitions and facts about abstract separation systems in these and any future papers using this framework.},
  langid = {english},
  keywords = {Connectivity,Graph,Lattice,Partial order,Tangle,Tree},
  file = {/Users/almico/Zotero/storage/UPHC4FBL/Diestel - 2018 - Abstract Separation Systems.pdf}
}

@article{diestelTanglesSocialSciences2019,
  title = {Tangles in the Social Sciences},
  author = {Diestel, Reinhard},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07341 [physics]},
  eprint = {1907.07341},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Traditional clustering identifies groups of objects that share certain qualities. Tangles do the converse: they identify groups of qualities that often occur together. They can thereby identify and discover 'types': of behaviour, views, abilities, dispositions. The mathematical theory of tangles has its origins in the connectivity theory of graphs, which it has transformed over the past 30 years. It has recently been axiomatized in a way that makes its two deepest results applicable to a much wider range of contexts. This expository paper indicates some contexts where this difference of approach is particularly striking. But these are merely examples of such contexts: in principle, it can apply to much of the quantitative social sciences. Our aim here is twofold: to indicate just enough of the theory of tangles to show how this can work in the various different contexts, and to give plenty of different examples illustrating this.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Mathematics - Combinatorics,Physics - Physics and Society},
  file = {/Users/almico/Zotero/storage/F8C3SIUJ/Diestel - 2019 - Tangles in the social sciences.pdf;/Users/almico/Zotero/storage/RHFW9947/1907.html}
}

@inproceedings{ghoshdastidarFoundationsComparisonBasedHierarchical2019,
  title = {Foundations of {{Comparison-Based Hierarchical Clustering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ghoshdastidar, Debarghya and Perrot, Micha{\"e}l and {von Luxburg}, Ulrike},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form objects i and j are more similar than objects k and l.'' Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.},
  file = {/Users/almico/Zotero/storage/EJTJBVMW/Ghoshdastidar et al. - 2019 - Foundations of Comparison-Based Hierarchical Clust.pdf}
}

@inproceedings{ghoshLandmarkOrdinalEmbedding2019,
  title = {Landmark {{Ordinal Embedding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ghosh, Nikhil and Chen, Yuxin and Yue, Yisong},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In this paper, we aim to learn a low-dimensional Euclidean representation from a set of constraints of the form ``item j is closer to item i than item k''. Existing approaches for this ``ordinal embedding'' problem require expensive optimization procedures, which cannot scale to handle increasingly larger datasets. To address this issue, we propose a landmark-based strategy, which we call Landmark Ordinal Embedding (LOE). Our approach trades off statistical efficiency for computational efficiency by exploiting the low-dimensionality of the latent embedding. We derive bounds establishing the statistical consistency of LOE under the popular Bradley- Terry-Luce noise model. Through a rigorous analysis of the computational complexity, we show that LOE is significantly more efficient than conventional ordinal embedding approaches as the number of items grows. We validate these characterizations empirically on both synthetic and real datasets. We also present a practical approach that achieves the ``best of both worlds'', by using LOE to warm-start existing methods that are more statistically efficient but computationally expensive.},
  file = {/Users/almico/Zotero/storage/LM8HU5ZK/Ghosh et al. - 2019 - Landmark Ordinal Embedding.pdf}
}

@misc{haghiriComparisonBasedFrameworkPsychophysics2019,
  title = {Comparison-{{Based Framework}} for {{Psychophysics}}: {{Lab}} versus {{Crowdsourcing}}},
  shorttitle = {Comparison-{{Based Framework}} for {{Psychophysics}}},
  author = {Haghiri, Siavash and Rubisch, Patricia and Geirhos, Robert and Wichmann, Felix and {von Luxburg}, Ulrike},
  year = {2019},
  month = jul,
  eprint = {1905.07234},
  eprinttype = {arxiv},
  abstract = {Traditionally, psychophysical experiments are conducted by repeated measurements on a few well-trained participants under well-controlled conditions, often resulting in, if done properly, high quality data. In recent years, however, crowdsourcing platforms are becoming increasingly popular means of data collection, measuring many participants at the potential cost of obtaining data of worse quality. In this paper we study whether the use of comparison-based (ordinal) data, combined with machine learning algorithms, can boost the reliability of crowdsourcing studies for psychophysics, such that they can achieve performance close to a lab experiment. To this end, we compare three setups: simulations, a psychophysics lab experiment, and the same experiment on Amazon Mechanical Turk. All these experiments are conducted in a comparison-based setting where participants have to answer triplet questions of the form "is object x closer to y or to z?". We then use machine learning to solve the triplet prediction problem: given a subset of triplet questions with corresponding answers, we predict the answer to the remaining questions. Considering the limitations and noise on MTurk, we find that the accuracy of triplet prediction is surprisingly close---but not equal---to our lab study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/QU6EWT9K/Haghiri et al. - 2019 - Comparison-Based Framework for Psychophysics Lab .pdf;/Users/almico/Zotero/storage/ILA3Q2WA/1905.html}
}

@article{haghiriEstimationPerceptualScales2020,
  title = {Estimation of Perceptual Scales Using Ordinal Embedding},
  author = {Haghiri, Siavash and Wichmann, Felix A. and {von Luxburg}, Ulrike},
  year = {2020},
  month = sep,
  journal = {Journal of Vision},
  volume = {20},
  number = {9},
  pages = {14--14},
  issn = {1534-7362},
  doi = {10.1167/jov.20.9.14},
  abstract = {In this article, we address the problem of measuring and analyzing sensation, the subjective magnitude of one's experience. We do this in the context of the method of triads: The sensation of the stimulus is evaluated via relative judgments of the following form: ``Is stimulus \textbackslash (S\_i\textbackslash ) more similar to stimulus \textbackslash (S\_j\textbackslash ) or to stimulus \textbackslash (S\_k\textbackslash )?'' We propose to use ordinal embedding methods from machine learning to estimate the scaling function from the relative judgments. We review two relevant and well-known methods in psychophysics that are partially applicable in our setting: nonmetric multidimensional scaling (NMDS) and the method of maximum likelihood difference scaling (MLDS). Considering various scaling functions, we perform an extensive set of simulations to demonstrate the performance of the ordinal embedding methods. We show that in contrast to existing approaches, our ordinal embedding approach allows, first, to obtain reasonable scaling functions from comparatively few relative judgments and, second, to estimate multidimensional perceptual scales. In addition to the simulations, we analyze data from two real psychophysics experiments using ordinal embedding methods. Our results show that in the one-dimensional perceptual scale, our ordinal embedding approach works as well as MLDS, while in higher dimensions, only our ordinal embedding methods can produce a desirable scaling function. To make our methods widely accessible, we provide an R-implementation and general rules of thumb on how to use ordinal embedding in the context of psychophysics.}
}

@inproceedings{huangVariationalPerspectiveDiffusionBased2021,
  title = {A {{Variational Perspective}} on {{Diffusion-Based Generative Models}} and {{Score Matching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, C. W. and Lim, J. H. and Courville, Aaron},
  year = {2021},
  month = may,
  abstract = {We derived an ELBO for continuous-time diffusion models using stochastic calculus, and made connection to continuous-time normalizing flows, hierarchical VAE, and score-based generative models.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/V8EN8NJ2/Huang et al. - 2021 - A Variational Perspective on Diffusion-Based Gener.pdf;/Users/almico/Zotero/storage/6FPZ4SU9/forum.html}
}

@phdthesis{inesschonmannSimilarityJudgementsNatural2021,
  title = {Similarity {{Judgements}} of {{Natural Images}}: {{Instructions Affect Observers}}' {{Decision Criteria}} and {{Consistency}}},
  author = {{In\'es Sch\"onmann}},
  year = {2021},
  month = sep,
  address = {{T\"ubingen}},
  school = {Universit\"at T\"ubingen}
}

@inproceedings{jainFiniteSamplePrediction2016,
  title = {Finite {{Sample Prediction}} and {{Recovery Bounds}} for {{Ordinal Embedding}}.},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Jain, Lalit and Jamieson, Kevin G. and Nowak, Robert D.},
  year = {2016},
  pages = {2703--2711},
  file = {/Users/almico/Zotero/storage/XSIQJ893/Jain et al. - 2016 - Finite Sample Prediction and Recovery Bounds for O.pdf;/Users/almico/Zotero/storage/X55FLHL8/1606.html}
}

@inproceedings{kleindessnerKernelFunctionsBased2017,
  title = {Kernel Functions Based on Triplet Comparisons},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kleindessner, Matth{\"a}us and {von Luxburg}, Ulrike},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Given only information in the form of similarity triplets "Object A is more similar to object B than to object C" about a data set, we propose two ways of defining a kernel function on the data set. While previous approaches construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining kernel functions that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply any kernel method to the data set.},
  file = {/Users/almico/Zotero/storage/DQXURHM7/Kleindessner and von Luxburg - 2017 - Kernel functions based on triplet comparisons.pdf}
}

@article{kleindessnerLensDepthFunction2017,
  title = {Lens {{Depth Function}} and K-{{Relative Neighborhood Graph}}: {{Versatile Tools}} for {{Ordinal Data Analysis}}},
  shorttitle = {Lens {{Depth Function}} and K-{{Relative Neighborhood Graph}}},
  author = {Kleindessner, Matth{\"a}us and von Luxburg, Ulrike},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {58},
  pages = {1--52},
  issn = {1533-7928},
  abstract = {In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as d(A,B)},
  file = {/Users/almico/Zotero/storage/K54XZVCL/Kleindessner and Luxburg - 2017 - Lens Depth Function and k-Relative Neighborhood Gr.pdf}
}

@misc{klepperClusteringTanglesAlgorithmic2021,
  title = {Clustering with {{Tangles}}: {{Algorithmic Framework}} and {{Theoretical Guarantees}}},
  shorttitle = {Clustering with {{Tangles}}},
  author = {Klepper, Solveig and Elbracht, Christian and Fioravanti, Diego and Kneip, Jakob and Rendsburg, Luca and Teegen, Maximilian and {von Luxburg}, Ulrike},
  year = {2020},
  month = jun,
  number = {arXiv:2006.14444},
  eprint = {2006.14444},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.14444},
  abstract = {Originally, tangles had been invented as an abstract tool in mathematical graph theory to prove the famous graph minor theorem. In this paper we showcase the practical potential of tangles in machine learning applications. Given a collection of weak cuts (bipartitions) of any dataset, tangles provide a mechanism to aggregate these cuts such that they point in the direction of a dense structure. As a result, a cluster is softly characterized by a set of consistent pointers. This highly flexible approach can solve soft clustering problems in a variety of setups, ranging from questionnaires over community detection in graphs to clustering points in metric spaces. The output of our proposed framework is of a hierarchical nature and induces the notion of a soft dendrogram, which can be helpful to explore the cluster structure of a dataset. The computational complexity of aggregating the cuts is only linear in the number of data points and places the bottleneck on generating the cuts, for which simple and fast algorithms form a sufficient basis. The contribution of this paper is to translate the abstract mathematical literature into algorithms that can be used in machine learning. We demonstrate the power of tangles in many different use cases and provide theoretical guarantees for their performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/STN9YBZE/Klepper et al. - 2021 - Clustering with Tangles Algorithmic Framework and.pdf;/Users/almico/Zotero/storage/7TAJMULF/2006.html}
}

@inproceedings{laurensvandermaatenStochasticTripletEmbedding2012,
  title = {Stochastic Triplet Embedding},
  booktitle = {{{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}}},
  author = {{L. van der Maaten} and {K. Weinberger}},
  year = {2012},
  month = sep,
  pages = {1--6},
  doi = {10.1109/MLSP.2012.6349720},
  isbn = {2378-928X}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\^bquanta,b=1,2, \textbackslash cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {/Users/almico/Zotero/storage/WAUQEET9/Lloyd - 1982 - Least squares quantization in PCM.pdf;/Users/almico/Zotero/storage/RJ6JSA85/1056489.html}
}

@inproceedings{roadsEnrichingImageNetHuman2021,
  title = {Enriching {{ImageNet}} with {{Human Similarity Judgments}} and {{Psychological Embeddings}}},
  booktitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Roads, Brett D. and Love, B.},
  year = {2021},
  doi = {10.1109/CVPR46437.2021.00355},
  abstract = {A publicly-available dataset that embodies the task-general capabilities of human perception and reasoning, and uses the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. Advances in supervised learning approaches to object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks\textemdash such as ILSVRC\textemdash are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of a large set of human similarity judgments that supplements the existing ILSVRC validation set. The new dataset supports a range of task and performance metrics, including evaluation of unsupervised algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. We were able to scale to the full 50,000 image ILSVRC validation set through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that the more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. ImageNet-HSJ supports the appraisal of internal representations and the development of more humanlike models.},
  file = {/Users/almico/Zotero/storage/9KRVI6AV/Roads and Love - 2021 - Enriching ImageNet with Human Similarity Judgments.pdf}
}

@article{robertsonGraphMinorsObstructions1991,
  title = {Graph Minors. {{X}}. {{Obstructions}} to Tree-Decomposition},
  author = {Robertson, Neil and Seymour, P. D},
  year = {1991},
  month = jul,
  journal = {Journal of Combinatorial Theory, Series B},
  volume = {52},
  number = {2},
  pages = {153--190},
  issn = {0095-8956},
  doi = {10.1016/0095-8956(91)90061-N},
  abstract = {Roughly, a graph has small ``tree-width'' if it can be constructed by piecing small graphs together in a tree structure. Here we study the obstructions to the existence of such a tree structure. We find, for instance: 1.(i) a minimax formula relating tree-width with the largest such obstructions2.(ii) an association between such obstructions and large grid minors of the graph3.(iii) a ``tree-decomposition'' of the graph into pieces corresponding with the obstructions. These results will be of use in later papers.},
  langid = {english},
  file = {/Users/almico/Zotero/storage/TJIXLF3X/Robertson and Seymour - 1991 - Graph minors. X. Obstructions to tree-decompositio.pdf;/Users/almico/Zotero/storage/SZVF968X/009589569190061N.html}
}

@article{semenovaStudyRashomonCurves2021,
  title = {A Study in {{Rashomon}} Curves and Volumes: {{A}} New Perspective on Generalization and Model Simplicity in Machine Learning},
  shorttitle = {A Study in {{Rashomon}} Curves and Volumes},
  author = {Semenova, L. and Rudin, C. and Parr, R.},
  year = {2021},
  month = apr,
  journal = {arXiv:1908.01755 [cs, stat]},
  eprint = {1908.01755},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Rashomon effect occurs when many different explanations exist for the same phenomenon. In machine learning, Leo Breiman used this term to characterize problems where many accurate-but-different models exist to describe the same data. In this work, we study how the Rashomon effect can be useful for understanding the relationship between training and test performance, and the possibility that simple-yet-accurate models exist for many problems. We consider the Rashomon set - the set of almost-equally-accurate models for a given problem - and study its properties and the types of models it could contain. We present the Rashomon ratio as a new measure related to simplicity of model classes, which is the ratio of the volume of the set of accurate models to the volume of the hypothesis space; the Rashomon ratio is different from standard complexity measures from statistical learning theory. For a hierarchy of hypothesis spaces, the Rashomon ratio can help modelers to navigate the trade-off between simplicity and accuracy. In particular, we find empirically that a plot of empirical risk vs. Rashomon ratio forms a characteristic \$\textbackslash Gamma\$-shaped Rashomon curve, whose elbow seems to be a reliable model selection criterion. When the Rashomon set is large, models that are accurate - but that also have various other useful properties - can often be obtained. These models might obey various constraints such as interpretability, fairness, or monotonicity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/9ARSVZCD/Semenova et al. - 2021 - A study in Rashomon curves and volumes A new pers.pdf;/Users/almico/Zotero/storage/4L6Z52IW/1908.html}
}

@article{shepardAnalysisProximitiesMultidimensional1962,
  title = {The Analysis of Proximities: {{Multidimensional}} Scaling with an Unknown Distance Function. {{II}}},
  shorttitle = {The Analysis of Proximities},
  author = {Shepard, Roger N.},
  year = {1962},
  month = sep,
  journal = {Psychometrika},
  volume = {27},
  number = {3},
  pages = {219--246},
  issn = {1860-0980},
  doi = {10.1007/BF02289621},
  abstract = {The first in the present series of two papers described a computer program for multidimensional scaling on the basis of essentially nonmetric data. This second paper reports the results of two kinds of test applications of that program. The first application is to artificial data generated by monotonically transforming the interpoint distances in a known spatial configuration. The purpose is to show that the recovery of the original metric configuration does not depend upon the particular transformation used. The second application is to measures of interstimulus similarity and confusability obtained from some actual psychological experiments.},
  langid = {english},
  keywords = {Computer Program,Distance Function,Present Series,Public Policy,Statistical Theory}
}

@article{stewartAbsoluteIdentificationRelative2005,
  title = {Absolute Identification by Relative Judgment},
  author = {Stewart, Neil and Brown, Gordon D. A. and Chater, Nick},
  year = {2005},
  month = oct,
  journal = {Psychological Review},
  volume = {112},
  number = {4},
  pages = {881--911},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.112.4.881},
  abstract = {In unidimensional absolute identification tasks, participants identify stimuli that vary along a single dimension. Performance is surprisingly poor compared with discrimination of the same stimuli. Existing models assume that identification is achieved using long-term representations of absolute magnitudes. The authors propose an alternative relative judgment model (RJM) in which the elemental perceptual units are representations of the differences between current and previous stimuli. These differences are used, together with the previous feedback, to respond. Without using long-term representations of absolute magnitudes, the RJM accounts for (a) information transmission limits, (b) bowed serial position effects, and (c) sequential effects, where responses are biased toward immediately preceding stimuli but away from more distant stimuli (assimilation and contrast).},
  langid = {english},
  pmid = {16262472},
  keywords = {Adult,Female,Humans,Judgment,Male,Models; Psychological},
  file = {/Users/almico/Zotero/storage/U6AWM55C/Stewart et al. - 2005 - Absolute identification by relative judgment.pdf}
}

@inproceedings{tamuzAdaptivelyLearningCrowd2011,
  title = {Adaptively Learning the Crowd Kernel},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Tamuz, Omer and Liu, Ce and Belongie, Serge and Shamir, Ohad and Kalai, Adam Tauman},
  year = {2011},
  month = jun,
  pages = {673--680},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {We introduce an algorithm that, given n objects, learns a similarity matrix over all n2 pairs, from crowdsourced data alone. The algorithm samples responses to adaptively chosen triplet-based relative-similarity queries. Each query has the form "is object a more similar to b or to c?" and is chosen to be maximally informative given the preceding responses. The output is an embedding of the objects into Euclidean space (like MDS); we refer to this as the "crowd kernel." SVMs reveal that the crowd kernel captures prominent and subtle features across a number of domains, such as "is striped" among neckties and "vowel vs. consonant" among letters.},
  isbn = {978-1-4503-0619-5}
}

@inproceedings{teradaLocalOrdinalEmbedding2014,
  title = {Local Ordinal Embedding},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Terada, Y. and Luxburg, U.},
  year = {2014},
  month = jan,
  volume = {3},
  pages = {2440--2458},
  abstract = {We study the problem of ordinal embedding: given a set of ordinal constraints of the form distance(i,j) {$<$} distance(k,l) for some quadruples (i,j, k, I) of indices, the goal is to construct a point configuration x1,..., xn in Rp that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embed-ding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know "local ordinal information" in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing. Copyright \textcopyright{} (2014) by the International Machine Learning Society (IMLS) All rights reserved.}
}

@inproceedings{ukkonenCrowdsourcedCorrelationClustering2017,
  title = {Crowdsourced {{Correlation Clustering}} with {{Relative Distance Comparisons}}},
  booktitle = {International {{Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Ukkonen, Antti},
  year = {2017},
  month = nov,
  pages = {1117--1122},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2017.148},
  abstract = {Crowdsourced, or human computation based clustering algorithms usually rely on relative distance comparisons, as these are easier to elicit from human workers than absolute distance information. We build upon existing work on correlation clustering, a well-known non-parametric approach to clustering, and present a novel clustering algorithm for human computation. We first define a novel variant of correlation clustering that is based on relative distance comparisons, and briefly outline an approximation algorithm for this problem. As a second contribution, we propose a more practical algorithm, which we empirically compare against existing methods from literature. Experiments with synthetic data suggest that our approach can outperform more complex methods. Also, our method efficiently finds good and intuitive clusterings from real relative distance comparison data.},
  keywords = {Algorithm design and analysis,Approximation algorithms,Clustering algorithms,Correlation,Optimized production technology,Search problems},
  file = {/Users/almico/Zotero/storage/VPX5GIHN/footnotes.html}
}

@article{vankadaraInsightsOrdinalEmbedding2021,
  title = {Insights into {{Ordinal Embedding Algorithms}}: {{A Systematic Evaluation}}},
  shorttitle = {Insights into {{Ordinal Embedding Algorithms}}},
  author = {Vankadara, Leena Chennuru and Haghiri, Siavash and Lohaus, Michael and Wahab, Faiz Ul and {von Luxburg}, Ulrike},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01666},
  eprint = {1912.01666},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.01666},
  abstract = {The objective of ordinal embedding is to find a Euclidean representation of a set of abstract items, using only answers to triplet comparisons of the form "Is item \$i\$ closer to the item \$j\$ or item \$k\$?". In recent years, numerous algorithms have been proposed to solve this problem. However, there does not exist a fair and thorough assessment of these embedding methods and therefore several key questions remain unanswered: Which algorithms perform better when the embedding dimension is constrained or few triplet comparisons are available? Which ones scale better with increasing sample size or dimension? In our paper, we address these questions and provide the first comprehensive and systematic empirical evaluation of existing algorithms as well as a new neural network approach. We find that simple, relatively unknown, non-convex methods consistently outperform all other algorithms, including elaborate approaches based on neural networks or landmark approaches. This finding can be explained by our insight that many of the non-convex optimization approaches do not suffer from local optima. Our comprehensive assessment is enabled by our unified library of popular embedding algorithms that leverages GPU resources and allows for fast and accurate embeddings of millions of data points.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/almico/Zotero/storage/5NBZ6E8V/Vankadara et al. - 2021 - Insights into Ordinal Embedding Algorithms A Syst.pdf;/Users/almico/Zotero/storage/JKM47DNV/1912.html}
}

@inproceedings{xuHowNeuralNetworks2022,
  title = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  shorttitle = {How {{Neural Networks Extrapolate}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Xu, Keyulu and Zhang, Mozhi and Li, Jingling and Du, Simon Shaolei and Kawarabayashi, Ken-Ichi and Jegelka, Stefanie},
  year = {2022},
  month = feb,
  abstract = {We study how neural networks trained by gradient descent  extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when...},
  langid = {english},
  file = {/Users/almico/Zotero/storage/WKGJTSAX/Xu et al. - 2022 - How Neural Networks Extrapolate From Feedforward .pdf;/Users/almico/Zotero/storage/9UI4RVGM/forum.html}
}


