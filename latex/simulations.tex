\chapter{Simulations}\label{simulations}
In this chapter and the following one, we want to explore how tangles perform on triplet data with the methods we proposed in \autoref{methods}. 
At first, we focus on simulations, as this allows us to control our data precisely. 
We will show in which cases Tangles perform well, in which ones they don't and what to keep in mind when applying the algorithm. \\

As datasets, we have decided on a mixture of gaussians and a hierarchical setup. In both cases, we will then generate triplets from the data points.
The gaussian setup serves as first baseline: it is a de-facto standard in clustering and a lot of real-life data is approximately gaussian. 
We have decided against using more complex data sets such as two-moons, as the focus lies on how the algorithms
interact with the triplets generated on the data and deal with additional complexities in that domain (such as triplets being corrupted, triplets missing, et cetera...). \\

The hierarchical setup takes the form of a noisy hierarchical block matrix, introduced by \cite{balakrishnanNoiseThresholdsSpectral2011}.
We use it to illustrate a second property of Tangles: in addition to pure clustering, we also produce a hierarchical tree, 
which can be used for hierarchical clustering. 

\section{Terms and methods used}
To evaluate the performance of the Tangles algorithm, we will need some metrics and other methods to use as a baseline. To compare a clustering against a ground truth,
we will use the \textit{normalized mutual information score} (NMI), which is independent of the cluster labels. 
$1.0$ indicates the same clustering (up to label permutations), $0.0$ indicates absolutely no mutual information between the clusterings (such as when our prediction
puts all data points in a single cluster). To compare the quality of different hierarchies, we make use of the \textit{average adjusted rand index} (AARI), 
introduced by \cite{ghoshdastidarFoundationsComparisonBasedHierarchical2019}. The AARI extends the \textit{adjusted rand index} (ARI), which is a clustering performance measurement
similar to the NMI, to compare hierarchies. In AARI, we calculate the ARI over all levels of the hierarchies we want to compare and average over all the obtained scores. 
% A further discussion on how we use the AARI for Tangles can be found in \label{sec:hierarchical_data}.


To generate the triplets, we first take the data and calculate a (dis)similarity on it, for example the euclidean distance between two data points. This allows
us to determine whether $(a,b,c)$ or $(a,c,b)$.
Then, we can use two approaches of drawing triplets. The first one is sampling triplets randomly and uniformly from the set of all triplets. The second one is a landmark approach:
we fix two randomly sampled points $a,b$, with $a \neq b$, sample all triplet that have the form $(x,a,b)$ (or equivalently $(x,b,a)$.  For the Landmark Tangles, only the 
second kind of triplets are useable, as discussed in \autoref{methods}, thus we will mostly stick to this format. 
We now have to differ between two possible ways of altering the triplets. We can add \textit{noise}
to the triplets. If we have a noise level of $p$, then every triplet is flipped with probability $p$, $(a,b,c)$ would be turned into $(a,c,b)$. We can also reduce
the \textit{density}, meaning that instead of sampling all triplets, we sample only a fraction $d$ of all triplets. In this section, when we use the term density, we
will refer to triplets sampled in a landmark approach, while when explicitly using the number of drawn triplets, we refer to a uniform sampling.

As a baseline, we will be using a combination of ordinal embedding together with a clustering algorithm. 
We first use an ordinal embedding algorithm to get an embedding of the triplet data. This approach has also been used in \cite{kleindessnerLensDepthFunction2017}. 
It has the advantage of also working with triplet data as opposed to clustering on the data directly, giving a more fair baseline, while still being very straightforward.
Afterwards, we use a standard clustering algorithm for euclidean data on the obtained embedding.  
There exist numerous algorithms for ordinal embedding (see \cite{vankadaraInsightsOrdinalEmbedding2021} for an overview) and for clustering.

For the ordinal embeddings, we have used Soft Ordinal Embedding (SOE \cite{teradaLocalOrdinalEmbedding2014}), as this has been identified by \cite{vankadaraInsightsOrdinalEmbedding2021} as one of the 
top-performing ordinal embedding algorithm for a variety of use cases. When testing out different baselines, we have also found SOE to consistently have top performance among 
all tested ordinal embedding algorithms. Additionally, we have included T-Stochatic Triplet Embedding (T-STE \cite{laurensvandermaatenStochasticTripletEmbedding2012}) 
to have a second baseline. For the clustering, have decided to go with k-Means, as one of the most basic clustering algorithms with usually good performance.

Additionally, we have included ComparisonHC \citep{ghoshdastidarFoundationsComparisonBasedHierarchical2019} as another baseline. This is another algorithm that doesn't
construct an embedding before clustering, and thus might allow for a more fair comparison to the tangles algorithm. Especially in the setting of a mixture of gaussians, 
the fact ordinal embedding algorithms aim to reconstruct a euclidean embedding seems to already introduce some bias that might benefit the ordinal embedding algorithms.
Normally, ComparisonHC is a hierachical clustering algorithm, which produces a dendrogram as its output. To use it as a baseline for clustering, we simply
cut the dendrogram off such that it produces the desired amount of clusters.

In the experiment figures, we will  use the abbreviations L-Tangles for Landmark Tangles, and M-Tangles for Majority Tangles.

\section{Gaussian data}\label{sec:gaussian_data}
\subsection{Experimental setup}
We generate a mixture of gaussians as follows: We draw a number of points $n$ each from $k$ different gaussian distributions with means $\mu_1, \mu_2, \ldots \mu_k$ and 
variances $\nu_1, \nu_2, \ldots \nu_k$. Each point gets assigned a label $y_i$ that corresponds to the number $i$ of the gaussian distribution it was drawn from.
For all of the experiments, unless mentioned otherwise, we use $n=20$, $k=3$, fixed means $\mu_1 = \begin{bmatrix} -6.0 & 3.0 \end{bmatrix}, 
\mu_2 = \begin{bmatrix} -6.0 & -3.0 \end{bmatrix},  \mu_2 = \begin{bmatrix} 6.0 & 3.0 \end{bmatrix}$ and a constant variance for all distributions of $\nu = I$, 
with $I$ as the identity matrix. A plot of the data set can be seen in \autoref{fig:dataset-gauss}. We generate the triplets via the euclidean distance between the points, so that
the triplet $(a,b,c)$ implies that $\|a - b\| < \|a - c\|$. For the Tangles algorithms, we use an agreement of $a=7$ (around 1/3 the size of the smallest
clusters we want to detect, in accordance with \cite{klepperClusteringTanglesAlgorithmic2021}), and a radius of $r=\frac{1}{3}$, for the majority
tangles, such that the cuts roughly have the diameter of the clusters.

\begin{figure}[h]
    \centering
    \resizebox{0.8\textwidth}{!}{\input{figures/results/gaussian_small_tangles_clustering.pgf}}
    \caption{}
    \label{fig:dataset-gauss}
    \caption{An example draw of the gaussian mixture used for our experiments.}
\end{figure}

\subsection{Lowering density}\label{lower_density}
First, we want to observe how our methods behave under different numbers of triplets present. As the number of triplet grows on the order of $O(N^3)$, it is only feasible to obtain
all triplets for very small datasets. Even with $N = n \cdot k = 60$, as in our case, there are already $106200$ triplets possible. If we imagine that the triplets have to be obtained 
through experiments with real people (such as in psychophysics settings), we might be able to get a few thouand triplets at most. If an algorithm performs better with a lower amount
of triplets, this can quickly translate into really large time, labor and money savings. To test this, we have drawn an increasing amount of triplets from our dataset in a landmark format.
We have plotted the results in \autoref{fig:lower_density_small}. We have also repeated this experiment for a larger number of datapoints, as this is a particularly interesting case. 
Usually, the larger the dataset, the smaller the percentage of all triplets we use, as it grows with $O(N^3)$. However, ordinal embedding algorithms empirically perform well with a lot
less triplets (for example requiring on the order of $O(n d \log(n))$ for euclidean data \citep{jainFiniteSamplePrediction2016}). 
Ideally, we would like for the Tangles algorithm to behave similarly. To test this, we have repeated the experiment with $n=200$ in \autoref{fig:density-change}, and lowered densities.
All other parameters were kept the same.

When looking at the plots, we can see that L-Tangles is performing at least as well as SOE, and even significantly better for a lot of densities in the case of $n=200$. 
As we would have expected, T-STE performs about as well as SOE, albeit a bit worse (and interestingly a lot worse for the larger dataset). ComparisonHC and M-Tangles
perform about the same level, but both stay far behind L-Tangles and SOE. With the larger dataset, we couldn't test ComparisonHC, as our obtained implementation requires 
constructing a $n^4$ matrix during the training step (this could possibly be remedied using a different implementation).

\onecolumn
\begin{figure}[ht]
    \centering
    \subfloat[20 points per cluster]{%
    \resizebox{0.5\textwidth}{!}{\input{figures/results/lower_density_small.pgf}}
    }
    \subfloat[200 points per cluster]{%
    \resizebox{0.5\textwidth}{!}{\input{figures/results/lower_density_large.pgf}}
    }
    \caption{
        We plot the NMI of different clustering method against the percentage of the triplets generated from a draw of a gaussian mixture with $3$ clusters. 
        We draw $20$ data points from each cluster for the left plot, and $200$ data points for the right plot 
        On the x-axis we have the density, where a density of $0.1$ means that we only use 10\% of the total number of triplets. The embedding methods (SOE, T-STE) are 
        followed by k-Means. The Tangles methods (L-Tangles, M-Tangles), are applied with $a=7$. ComparisonHC was left out of the right plot due to computational issues.}
    \label{fig:density-change}
\end{figure}


\subsection{Adding noise}\label{sec:adding-noise}
Next, we want to observe how our algorithm behaves under added noise. This is an important model: most applications of triplet data use triplets that are generated from
real humans. They might disagree on which objects are closer and which are not, which can be modelled as noise on the responses of our triplets. The higher the noise, 
the more disagreement is there about the similarities of objects, so it would matter more about which person you ask than which objects you present them. 
%TODO M-Tangles ComparisonHC
We have plotted our results in \autoref{fig:adding-noise}. 
We observe that L-Tangles falls off a lot quicker in performance than SOE, and falls off a bit harder than T-STE, but they are in the same area. We observe however, that until
relatively large noise levels ($>0.1$), all algorithms performs the same, meaning that L-Tangles can still perform well with low to medium levels of noise.

\begin{figure}[h]
    \centering
    \resizebox{0.7\textwidth}{!}{\input{figures/results/adding_noise_small.pgf}}
    \caption{}
    \label{fig:adding-noise}
    \caption{
        We plot the NMI of our chosen clustering methods against the noise that we introduce on the triplets.  We use $3$ clusters and $20$ data points per cluster, sampling all possible triplets. On a noise level of $0.1$, this means that we flip 10\% of the triplets around (turning for example $(a,b,c)$ to $(a,c,b)$). 
    }
\end{figure}

\subsection{Adding noise and lowering density}
In this experiment, we compare \autoref{sec:lowering-density} and \autoref{sec:adding-noise}. We have seen that L-Tangles can perform well with a low
amount of triplets (a bit better than SOE), and have reasonable performance for noisy triplets (worse than SOE). We are now interested to see how large the area is
where L-Tangles can still outperform SOE when we consider both a low density and noisy triplets, as these two factors are probably the most interesting variables
when choosing an algorithm to evaluate triplet data. We have generated two heatmaps, which can be found in \autoref{fig:noise-density-heatmaps}. 
There we can see that L-Tangles outperforms SOE in quite a large region in the low-noise , low-density regime. We would imagine this effect to be even larger
for a $n=200$ setup, but found this computationally too intensive. On the contrary, SOE performs better in the high-density, high-noise regions, with about similar performance
for the cases of low-noise high-density (perfect clustering) and high-noise low-density (random clustering).
% TODO: Repeat for n=200
% TODO: green border around the parts where L-Tangles outperforms?

\onecolumn
\begin{figure}[ht]
    \centering
    \subfloat[SOE]{%
    %\resizebox{0.5\textwidth}{!}{\input{figures/results/gaussian_small_lower_density_noise_heatmap_soe.pdf}}
        \resizebox{0.5\textwidth}{!}{\includegraphics{figures/results/gaussian_small_lower_density_noise_heatmap_soe.pdf}}
    }
    \subfloat[L-Tangles]{%
        \resizebox{0.5\textwidth}{!}{\includegraphics{figures/results/gaussian_small_lower_density_noise_heatmap_l_tangles.pdf}}
    }
    \caption{
        We plot a heatmap of the NMI of SOE and L-Tangles over changing noise and the density of our the triplets. 
        The regions with a darker shade of blue indicate better performance of the algorithm. 
        Note that the noise increases as we move down, and the
        density decreases as we move right.  We use $3$ clusters and $20$ data points per cluster, sampling all possible triplets. 
    }
    \label{fig:noise-density-heatmaps}
\end{figure}

\subsection{Missing triplets}
% TODO
In this experiment, we will use the small gaussian data set and gradually sample an increasing number of triplets for it. 
This is very similar to the setup in \autoref{lower_density}, but this time we sample triplets uniformly at random without replacement
from the set of all triplets. In this setup, we will not be able to use Landmark Tangles, as there will be missing values in the cuts. 
In principle, one could imagine using some kind of imputation to fill in the missing triplets. This is not an effective method,
as there are simply way too little triplets compared to the set of all triplets used. We have shown in \autoref{?} 
how Landmark Tangles behaves under different imputation methods with a decreasing number of triplets. One ca see that all methods fall 
off very quickly in performance, a lot faster than the other clustering methods.

In \autoref{?}, we have shown the performance of our other algorithms over the number of triplets present.


% TODO: Experiment that shows how quickly LT starts falling off, maybe with different imputations?
\subsection{The case of weird geometry}

\subsection{Discussion}

\section{Hierarchical data}\label{sec:hierarchical_data}
\subsection{Experimental setup}
We generate a noise hierarchical block matrix \citep{\cite{balakrishnanNoiseThresholdsSpectral2011}.
The dendrogram described by this model has the form of a complete binary tree, and the similarities of the data points are described via a similarity matrix $M$.
In this matrix the elements in the same cluster have the highest similarity $mu_0$, and for each level $l$ in the dendrogram that two classes are removed
from each other, their similarity decreases by $delta$. We can then add a noise matrix $R$ to the similarity matrix and receive the noisy hierarchical block matrix $M' = M + R$. 
In our setup, we use a noise matrix $R$ where every entry is simply drawn from a normal distribution with mean $0$ and standard deviation $\sigma$.
More about the generation process can be read in \cite{ghoshdastidarFoundationsComparisonBasedHierarchical2019}.
%TODO: Image of similarity matrix?
% can we make this bigger?
We choose a relatively simple setup of $4$ clusters with $10$ data points each, an initial class similarity $\mu_0 = 5$ and a similarity decrease of $\delta = 1$.
In this setup, there are two kinds of noise we encounter: the noise that is injected into the hierarchy itself via the noise matrix and the noise that is added to the triplets itself.
The triplet noise has the same form as in \autoref{sec:gaussian_data}, on noise $p$ we simply flip every triplet with probability $p$. We will investigate how our algorithm does
under both noise models, as well as under a lowered density.

When evaluating, we compare both the final clustering (the lowest level of the hierarchical block matrix) as well as the obtained hierarchy to each other.
% TODO: Explain more about the AARI?
%As Tangles does not produce a dendrogram, we might have a problem obtaining the desired amount of clusters in a certain level. In this case, we simply fill up 
%#For comparing the hierarchy, we can only use Tangles and ComparisonHC, as the embedding methods do not 

\subsection{Adding triplet noise}
Similar to the gaussian setup, we have simply increased the noise on the sampled triplets and have evaluated the performance of our algorithms.

\subsection{Adding hierarchy noise}

\subsection{Lowering density}

\subsection{Discussion}


